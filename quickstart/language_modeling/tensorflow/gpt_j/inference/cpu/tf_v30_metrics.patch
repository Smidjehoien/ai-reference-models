diff --git a/src/transformers/generation/tf_utils.py b/src/transformers/generation/tf_utils.py
index 5e4bc58c8..1bd6ef401 100644
--- a/src/transformers/generation/tf_utils.py
+++ b/src/transformers/generation/tf_utils.py
@@ -15,6 +15,7 @@
 # limitations under the License.
 
 import copy
+import time
 import inspect
 import warnings
 from dataclasses import dataclass
@@ -734,6 +735,7 @@ class TFGenerationMixin:
 
         # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call
         self._validate_model_class()
+        self.token_latency = kwargs.pop("token_latency", None)
 
         # priority: `generation_config` argument > `model.generation_config` (the default generation config)
         if generation_config is None:
@@ -794,7 +796,7 @@ class TFGenerationMixin:
             logger.warning(f"Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.")
             generation_config.pad_token_id = eos_token_id
 
-        use_xla = not tf.executing_eagerly()
+        use_xla = True #not tf.executing_eagerly()
         if use_xla and not self.supports_xla_generation:
             raise ValueError(
                 "The selected model does not support Graph mode nor XLA generation (e.g. from tf.function())"
@@ -2217,391 +2219,156 @@ class TFGenerationMixin:
         >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
         ['Wie alt bist du?']
         ```"""
+        a = tf.timestamp()
+        latency_list = []
+        latency_list.append(a)
+        with tf.control_dependencies([a]):
+            def flatten_beam_dim(tensor, batch_axis=0):
+                """Flattens the first two dimensions of a non-scalar array."""
+                shape = shape_list(tensor)
+                return tf.reshape(
+                    tensor,
+                    shape[:batch_axis] + [shape[batch_axis] * shape[batch_axis + 1]] + shape[batch_axis + 2 :],
+                )
 
-        def flatten_beam_dim(tensor, batch_axis=0):
-            """Flattens the first two dimensions of a non-scalar array."""
-            shape = shape_list(tensor)
-            return tf.reshape(
-                tensor,
-                shape[:batch_axis] + [shape[batch_axis] * shape[batch_axis + 1]] + shape[batch_axis + 2 :],
-            )
-
-        def unflatten_beam_dim(tensor, num_beams, batch_axis=0):
-            """Unflattens the first, flat batch*beam dimension of a non-scalar array."""
-            shape = shape_list(tensor)
-            return tf.reshape(tensor, shape[:batch_axis] + [-1, num_beams] + shape[batch_axis + 1 :])
-
-        # 1. init beam_search values
-        logits_processor = logits_processor if logits_processor is not None else TFLogitsProcessorList()
-        logits_warper = logits_warper if logits_warper is not None else TFLogitsProcessorList()
-
-        max_length = max_length if max_length is not None else self.generation_config.max_length
-        pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id
-        eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id
-        if isinstance(eos_token_id, int):
-            eos_token_id = [eos_token_id]
-        num_return_sequences = (
-            num_return_sequences if num_return_sequences is not None else self.generation_config.num_return_sequences
-        )
-
-        output_attentions = (
-            output_attentions if output_attentions is not None else self.generation_config.output_attentions
-        )
-        output_hidden_states = (
-            output_hidden_states if output_hidden_states is not None else self.generation_config.output_hidden_states
-        )
-        output_scores = output_scores if output_scores is not None else self.generation_config.output_scores
-        return_dict_in_generate = (
-            return_dict_in_generate
-            if return_dict_in_generate is not None
-            else self.generation_config.return_dict_in_generate
-        )
-
-        length_penalty = length_penalty if length_penalty is not None else self.generation_config.length_penalty
-        early_stopping = early_stopping if early_stopping is not None else self.generation_config.early_stopping
-
-        use_cache = model_kwargs.pop("use_cache", self.generation_config.use_cache)
-        use_xla = not tf.executing_eagerly()
-        # TODO (Joao): fix cache format or find programatic way to detect cache index
-        # GPT2 and other models has a slightly different cache structure, with a different batch axis
-        model_name = str(self.decoder) if "EncoderDecoder" in str(self) else str(self)
-        cache_batch_axis = 1 if any([model_prefix in model_name for model_prefix in ("TFGPT2", "TFCTRL")]) else 0
-        # some models, like XLNet, need more than the last token in the presence of past_key_values
-        needs_full_input = "use_mems" in set(inspect.signature(self.prepare_inputs_for_generation).parameters.keys())
-
-        # 2. init `attentions`, `hidden_states`, and `scores` tuples
-        all_scores = [] if (return_dict_in_generate and output_scores) else None
-        decoder_attentions = [] if (return_dict_in_generate and output_attentions) else None
-        cross_attentions = [] if (return_dict_in_generate and output_attentions) else None
-        decoder_hidden_states = [] if (return_dict_in_generate and output_hidden_states) else None
-
-        # 3. init tensors to use for "xla-compileable" generate function
-        batch_size, num_beams, cur_len = shape_list(input_ids)
-
-        # per batch, beam-item holding current token in loop, pre-populated with `pad_token_id`
-        input_ids_padding = tf.ones((batch_size, num_beams, max_length - cur_len), dtype=tf.int32) * (
-            pad_token_id or 0
-        )
-        running_sequences = tf.concat([input_ids, input_ids_padding], axis=-1)
-        sequences = tf.ones((batch_size, num_beams, max_length), dtype=tf.int32) * (pad_token_id or 0)
-
-        # per batch,beam-item state bit indicating if sentence has finished.
-        is_sent_finished = tf.zeros((batch_size, num_beams), dtype=tf.bool)
-
-        # per batch, beam-item score, logprobs
-        running_scores = tf.tile(
-            tf.expand_dims(tf.convert_to_tensor([0.0] + [-1.0e9] * (num_beams - 1)), axis=0), [batch_size, 1]
-        )
-        scores = tf.ones((batch_size, num_beams)) * -1.0e9
-
-        # per batch beam indices
-        running_beam_indices = tf.ones((batch_size, num_beams, max_length), dtype=tf.int32) * -1
-        beam_indices = tf.ones((batch_size, num_beams, max_length), dtype=tf.int32) * -1
-
-        # flatten beam dim
-        if "encoder_outputs" in model_kwargs:
-            model_kwargs["encoder_outputs"]["last_hidden_state"] = flatten_beam_dim(
-                model_kwargs["encoder_outputs"]["last_hidden_state"]
+            def unflatten_beam_dim(tensor, num_beams, batch_axis=0):
+                """Unflattens the first, flat batch*beam dimension of a non-scalar array."""
+                shape = shape_list(tensor)
+                return tf.reshape(tensor, shape[:batch_axis] + [-1, num_beams] + shape[batch_axis + 1 :])
+
+            # 1. init beam_search values
+            # tic = time.time()
+            dt = "fp32"
+            toDtype = tf.dtypes.float32
+            logits_processor = logits_processor if logits_processor is not None else TFLogitsProcessorList()
+            logits_warper = logits_warper if logits_warper is not None else TFLogitsProcessorList()
+
+            max_length = max_length if max_length is not None else self.generation_config.max_length
+            pad_token_id = pad_token_id if pad_token_id is not None else self.generation_config.pad_token_id
+            eos_token_id = eos_token_id if eos_token_id is not None else self.generation_config.eos_token_id
+            if isinstance(eos_token_id, int):
+                eos_token_id = [eos_token_id]
+            num_return_sequences = (
+                num_return_sequences if num_return_sequences is not None else self.generation_config.num_return_sequences
             )
-        if "attention_mask" in model_kwargs:
-            model_kwargs["attention_mask"] = flatten_beam_dim(model_kwargs["attention_mask"])
 
-        # 4. define "xla-compile-able" stop-condition and auto-regressive function
-        # define stop-condition and auto-regressive function
-        def beam_search_cond_fn(
-            cur_len,
-            running_sequences,
-            running_scores,
-            running_beam_indices,
-            sequences,
-            scores,
-            beam_indices,
-            is_sent_finished,
-            model_kwargs,
-        ):
-            """
-            Beam Search termination condition function -- halts the generation loop if any of these conditions becomes
-            False
-            """
-            # 1. is less than max length?
-            not_max_length_yet = cur_len < max_length
-
-            # 2. can the new beams still improve?
-            # early_stopping == False -> apply heuristic = always get the best score from `cur_len`. See the discussion
-            # below for more details.
-            # https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565
-            # early_stopping == "never" -> compute the best score from max_length or cur_len, depending on the sign of
-            #   length_penalty. Positive length_penalty favors longer sequences, thus we use max_length there.
-            if early_stopping == "never" and length_penalty > 0.0:
-                best_running_score = running_scores[:, :1] / (max_length**length_penalty)
-            else:
-                best_running_score = running_scores[:, :1] / (tf.cast(cur_len, dtype=tf.float32) ** length_penalty)
-            worst_finished_score = tf.where(
-                is_sent_finished, tf.math.reduce_min(scores, axis=1, keepdims=True), -1.0e9
+            output_attentions = (
+                output_attentions if output_attentions is not None else self.generation_config.output_attentions
             )
-            improvement_still_possible = tf.math.reduce_any(best_running_score > worst_finished_score)
-
-            # 3. is there still a beam that has not finished?
-            still_open_beam = ~(tf.math.reduce_all(is_sent_finished) & (early_stopping is True))
-
-            return not_max_length_yet & still_open_beam & improvement_still_possible
-
-        def beam_search_body_fn(
-            cur_len,
-            running_sequences,
-            running_scores,
-            running_beam_indices,
-            sequences,
-            scores,
-            beam_indices,
-            is_sent_finished,
-            model_kwargs,
-        ):
-            """
-            Beam Search iterative update function -- each iteration adds a new token and updates the best sequences
-            seen so far
-            """
-            # 1. Forward current tokens
-            if model_kwargs.get("past_key_values") is None or needs_full_input:
-                input_ids = running_sequences[:, :, :cur_len]
-            else:
-                input_ids = tf.expand_dims(running_sequences[:, :, cur_len - 1], -1)
-            model_inputs = self.prepare_inputs_for_generation(
-                flatten_beam_dim(input_ids), use_cache=use_cache, **model_kwargs
+            output_hidden_states = (
+                output_hidden_states if output_hidden_states is not None else self.generation_config.output_hidden_states
             )
-            model_outputs = self(
-                **model_inputs,
-                return_dict=True,
-                output_attentions=output_attentions,
-                output_hidden_states=output_hidden_states,
+            output_scores = output_scores if output_scores is not None else self.generation_config.output_scores
+            return_dict_in_generate = (
+                return_dict_in_generate
+                if return_dict_in_generate is not None
+                else self.generation_config.return_dict_in_generate
             )
-            logits = unflatten_beam_dim(model_outputs.logits[:, -1], num_beams)
-
-            # 2. Compute log probs
-            # get log probabilities from logits, process logits with processors (*e.g.* min_length, ...), and
-            # add new logprobs to existing running logprobs scores.
-            log_probs = tf.nn.log_softmax(logits)
-            log_probs = logits_processor(flatten_beam_dim(running_sequences), flatten_beam_dim(log_probs), cur_len)
-            log_probs = unflatten_beam_dim(log_probs, num_beams)
-            log_probs_processed = log_probs
-            log_probs = log_probs + tf.expand_dims(running_scores, axis=2)
-            if do_sample:
-                # Note: logits warpers are intentionally applied after adding running beam scores. On some logits
-                # warpers (like top_p) this is indiferent, but on others (like temperature) it is not. For reference,
-                # see https://github.com/huggingface/transformers/pull/5420#discussion_r449779867
-                log_probs = logits_warper(flatten_beam_dim(running_sequences), flatten_beam_dim(log_probs), cur_len)
-                log_probs = unflatten_beam_dim(log_probs, num_beams)
-            vocab_size = log_probs.shape[2]
-            log_probs = tf.reshape(log_probs, (batch_size, num_beams * vocab_size))
-
-            # Store scores, attentions and hidden_states when required
-            if not use_xla and return_dict_in_generate:
-                if output_scores:
-                    all_scores.append(
-                        logits_warper(
-                            flatten_beam_dim(running_sequences), flatten_beam_dim(log_probs_processed), cur_len
-                        )
-                    )
-                if output_attentions and self.config.is_encoder_decoder:
-                    decoder_attentions.append(model_outputs.decoder_attentions)
-                elif output_attentions and not self.config.is_encoder_decoder:
-                    decoder_attentions.append(model_outputs.attentions)
-                    if self.config.is_encoder_decoder:
-                        cross_attentions.append(model_outputs.cross_attentions)
 
-                if output_hidden_states and self.config.is_encoder_decoder:
-                    decoder_hidden_states.append(model_outputs.decoder_hidden_states)
-                elif output_hidden_states and self.config.is_encoder_decoder:
-                    decoder_hidden_states.append(model_outputs.hidden_states)
-
-            # 3. Retrieve top-K
-            # Each item in batch has num_beams * vocab_size candidate sequences. For each item, get the top 2*k
-            # candidates with the highest log-probabilities. We gather the top 2*K beams here so that even if the
-            # best K sequences reach EOS simultaneously, we have another K sequences remaining to continue the live
-            # beam search.
-            # Gather the top 2*K scores from _all_ beams.
-            # Gather 2*k top beams.
-            # Recover the beam index by floor division.
-            # Recover token id by modulo division and expand Id array for broadcasting.
-            # Update sequences for the 2*K top-k new sequences.
-            beams_to_keep = 2 * num_beams
-            if do_sample:
-                topk_indices = sample_without_replacement(log_probs, beams_to_keep)
-                topk_log_probs = tf.gather(log_probs, topk_indices, axis=1, batch_dims=1)
-            else:
-                topk_log_probs, topk_indices = tf.math.top_k(log_probs, k=beams_to_keep)
-            topk_current_beam_indices = topk_indices // vocab_size
-            topk_running_beam_indices = self._gather_beams(running_beam_indices, topk_current_beam_indices)
-            topk_running_sequences = self._gather_beams(running_sequences, topk_current_beam_indices)
-            topk_ids = topk_indices % vocab_size
-
-            # writes the new token
-            indices_batch = tf.repeat(tf.range(batch_size), [beams_to_keep])
-            indices_beam = tf.tile(tf.range(beams_to_keep), [batch_size])
-            update_indices = tf.stack(
-                [indices_batch, indices_beam, tf.broadcast_to(cur_len, [batch_size * beams_to_keep])], axis=-1
-            )
-            topk_sequences = tf.tensor_scatter_nd_update(
-                tensor=topk_running_sequences,
-                indices=update_indices,
-                updates=tf.reshape(topk_ids, [batch_size * beams_to_keep]),
+            length_penalty = length_penalty if length_penalty is not None else self.generation_config.length_penalty
+            early_stopping = early_stopping if early_stopping is not None else self.generation_config.early_stopping
+
+            use_cache = model_kwargs.pop("use_cache", self.generation_config.use_cache)
+            use_xla = True # not tf.executing_eagerly()
+            # TODO (Joao): fix cache format or find programatic way to detect cache index
+            # GPT2 and other models has a slightly different cache structure, with a different batch axis
+            model_name = str(self.decoder) if "EncoderDecoder" in str(self) else str(self)
+            cache_batch_axis = 1 if any([model_prefix in model_name for model_prefix in ("TFGPT2", "TFCTRL")]) else 0
+            # some models, like XLNet, need more than the last token in the presence of past_key_values
+            needs_full_input = "use_mems" in set(inspect.signature(self.prepare_inputs_for_generation).parameters.keys())
+
+            # 2. init `attentions`, `hidden_states`, and `scores` tuples
+            all_scores = [] if (return_dict_in_generate and output_scores) else None
+            decoder_attentions = [] if (return_dict_in_generate and output_attentions) else None
+            cross_attentions = [] if (return_dict_in_generate and output_attentions) else None
+            decoder_hidden_states = [] if (return_dict_in_generate and output_hidden_states) else None
+
+            # 3. init tensors to use for "xla-compileable" generate function
+            batch_size, num_beams, cur_len = shape_list(input_ids)
+
+            # per batch, beam-item holding current token in loop, pre-populated with `pad_token_id`
+            input_ids_padding = tf.ones((batch_size, num_beams, max_length - cur_len), dtype=tf.int32) * (
+                pad_token_id or 0
             )
+            running_sequences = tf.concat([input_ids, input_ids_padding], axis=-1)
+            sequences = tf.ones((batch_size, num_beams, max_length), dtype=tf.int32) * (pad_token_id or 0)
 
-            # we want to store the beam indices with batch information -> real beam index = beam index % num beams
-            batch_modified_indices = topk_current_beam_indices + tf.broadcast_to(
-                tf.expand_dims(tf.range(batch_size) * num_beams, axis=1), topk_current_beam_indices.shape
-            )
-            topk_beam_indices = tf.tensor_scatter_nd_update(
-                tensor=topk_running_beam_indices,
-                indices=update_indices,
-                updates=tf.reshape(batch_modified_indices, [batch_size * beams_to_keep]),
-            )
+            # per batch,beam-item state bit indicating if sentence has finished.
+            is_sent_finished = tf.zeros((batch_size, num_beams), dtype=tf.bool)
 
-            # 4. Check which sequences have ended
-            # Update current sequences: Did the top `num_beams` sequences reach an end marker?
-            # To prevent these just finished sequences from being added to the current sequences
-            # set of active beam search sequences, set their log probs to a very large negative value.
-            if eos_token_id is None:
-                eos_in_next_token = tf.zeros(topk_sequences[:, :, cur_len].shape, dtype=tf.bool)
-            else:
-                eos_in_next_token = tf.math.reduce_any(
-                    tf.equal(
-                        tf.broadcast_to(
-                            topk_sequences[:, :, cur_len], [len(eos_token_id)] + topk_sequences[:, :, cur_len].shape
-                        ),
-                        tf.expand_dims(tf.expand_dims(eos_token_id, -1), -1),
-                    ),
-                    axis=0,
-                )
-            did_topk_just_finished = eos_in_next_token & tf.broadcast_to(
-                tf.concat((tf.ones((num_beams), dtype=tf.bool), tf.zeros((num_beams), dtype=tf.bool)), axis=0),
-                shape_list(eos_in_next_token),
+            # per batch, beam-item score, logprobs
+            running_scores = tf.tile(
+                tf.expand_dims(tf.convert_to_tensor([0.0] + [-1.0e9] * (num_beams - 1)), axis=0), [batch_size, 1]
             )
-
-            # non-top `num_beams` eos tokens can't be used to finish a beam, but the others can't be used in the next
-            # running sentences either
-            running_topk_log_probs = topk_log_probs + tf.cast(eos_in_next_token, tf.float32) * -1.0e9
-
-            # 5. Get running sequences scores for next
-            # Determine the top k beam indices (from top 2*k beams) from log probs and gather top k beams
-            # (from top 2*k beams).
-            next_topk_indices = tf.math.top_k(running_topk_log_probs, k=num_beams)[1]
-            next_running_sequences, next_running_scores, next_running_beam_indices = self._gather_beams(
-                [topk_sequences, running_topk_log_probs, topk_beam_indices], next_topk_indices
-            )
-
-            # 6. Process topk logits
-            # Further process log probs:
-            # - add length penalty
-            # - make sure no scores can be added anymore if beam is full
-            # - make sure still running sequences cannot be chosen as finalized beam
-            topk_log_probs = topk_log_probs / (tf.cast(cur_len, dtype=tf.float32) ** length_penalty)
-            beams_in_batch_are_full = tf.broadcast_to(
-                tf.math.reduce_all(is_sent_finished, axis=-1, keepdims=True), shape_list(did_topk_just_finished)
-            ) & (early_stopping is True)
-            add_penalty = ~did_topk_just_finished | beams_in_batch_are_full
-            topk_log_probs += tf.cast(add_penalty, tf.float32) * -1.0e9
-
-            # 7. Get scores, sequences, is sentence finished for next.
-            # Combine sequences, scores, and flags along the beam dimension and compare new finished sequence scores
-            # to existing finished scores and select the best from the new set of beams
-            merged_sequences = tf.concat([sequences, topk_sequences], axis=1)
-            merged_scores = tf.concat([scores, topk_log_probs], axis=1)
-            merged_beams = tf.concat([beam_indices, topk_beam_indices], axis=1)
-            merged_is_sent_finished = tf.concat([is_sent_finished, did_topk_just_finished], axis=1)
-            topk_merged_indices = tf.math.top_k(merged_scores, k=num_beams)[1]
-            next_sequences, next_scores, next_beam_indices, next_is_sent_finished = self._gather_beams(
-                [merged_sequences, merged_scores, merged_beams, merged_is_sent_finished], topk_merged_indices
-            )
-
-            # 8. Prepare data for the next iteration
-            # Determine the top k beam indices from the original set of all beams. With these, gather the top k
-            # beam-associated caches.
-            cur_len = cur_len + 1
-            if "past_key_values" in model_outputs:
-                cache = tf.nest.map_structure(
-                    lambda tensor: unflatten_beam_dim(tensor, num_beams, batch_axis=cache_batch_axis),
-                    model_outputs.past_key_values,
-                )
-                next_running_indices = self._gather_beams(topk_current_beam_indices, next_topk_indices)
-                next_cache = self._gather_beams(cache, next_running_indices, batch_axis=cache_batch_axis)
-                model_outputs["past_key_values"] = tf.nest.map_structure(
-                    lambda tensor: flatten_beam_dim(tensor, batch_axis=cache_batch_axis), next_cache
+            scores = tf.ones((batch_size, num_beams)) * -1.0e9
+
+            if tf.keras.mixed_precision.global_policy().get_config()["name"] == "mixed_bfloat16":
+                dt = "bf16"
+                toDtype = tf.dtypes.bfloat16
+            elif tf.keras.mixed_precision.global_policy().get_config()["name"] == "mixed_float16":
+                dt = "fp16"
+                toDtype = tf.dtypes.float16
+            
+            if dt != "fp32":
+                running_scores = tf.cast(running_scores, dtype=toDtype)
+                scores = tf.cast(scores, dtype=toDtype)
+
+            # per batch beam indices
+            running_beam_indices = tf.ones((batch_size, num_beams, max_length), dtype=tf.int32) * -1
+            beam_indices = tf.ones((batch_size, num_beams, max_length), dtype=tf.int32) * -1
+
+            # flatten beam dim
+            if "encoder_outputs" in model_kwargs:
+                model_kwargs["encoder_outputs"]["last_hidden_state"] = flatten_beam_dim(
+                    model_kwargs["encoder_outputs"]["last_hidden_state"]
                 )
+            if "attention_mask" in model_kwargs:
+                model_kwargs["attention_mask"] = flatten_beam_dim(model_kwargs["attention_mask"])
 
-            if use_xla:
-                next_model_kwargs = self._update_model_kwargs_for_xla_generation(
-                    model_outputs=model_outputs,
-                    model_kwargs=model_kwargs,
-                    cur_len=cur_len,
-                    max_length=max_length,
-                    batch_size=(batch_size * num_beams),
-                    is_encoder_decoder=self.config.is_encoder_decoder,
-                    batch_axis=cache_batch_axis,
-                )
-            else:
-                next_model_kwargs = self._update_model_kwargs_for_generation(
-                    model_outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder
+            # 4. define "xla-compile-able" stop-condition and auto-regressive function
+            # define stop-condition and auto-regressive function
+            def beam_search_cond_fn(
+                tt,
+                cur_len,
+                running_sequences,
+                running_scores,
+                running_beam_indices,
+                sequences,
+                scores,
+                beam_indices,
+                is_sent_finished,
+                model_kwargs,
+            ):
+                """
+                Beam Search termination condition function -- halts the generation loop if any of these conditions becomes
+                False
+                """
+                # 1. is less than max length?
+                not_max_length_yet = cur_len < max_length
+
+                # 2. can the new beams still improve?
+                # early_stopping == False -> apply heuristic = always get the best score from `cur_len`. See the discussion
+                # below for more details.
+                # https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565
+                # early_stopping == "never" -> compute the best score from max_length or cur_len, depending on the sign of
+                #   length_penalty. Positive length_penalty favors longer sequences, thus we use max_length there.
+                if early_stopping == "never" and length_penalty > 0.0:
+                    best_running_score = running_scores[:, :1] / (max_length**length_penalty)
+                else:
+                    best_running_score = running_scores[:, :1] / tf.cast((tf.cast(cur_len, dtype=tf.float32) ** length_penalty), toDtype)
+                worst_finished_score = tf.where(
+                    is_sent_finished, tf.math.reduce_min(scores, axis=1, keepdims=True), -1.0e9
                 )
+                improvement_still_possible = tf.math.reduce_any(best_running_score > worst_finished_score)
 
-                # if we don't cache past_key_values key values we need the whole input
-                if model_kwargs.get("past_key_values", None) is None:
-                    # let's throw out `past_key_values` since we don't want `None` tensors
-                    model_kwargs.pop("past_key_values", None)
+                # 3. is there still a beam that has not finished?
+                still_open_beam = ~(tf.math.reduce_all(is_sent_finished) & (early_stopping is True))
 
-            return (
-                cur_len,
-                next_running_sequences,
-                next_running_scores,
-                next_running_beam_indices,
-                next_sequences,
-                next_scores,
-                next_beam_indices,
-                next_is_sent_finished,
-                next_model_kwargs,
-            )
+                return not_max_length_yet & still_open_beam & improvement_still_possible
 
-        # 5. run generation
-        # 1st generation step has to be run before to initialize `past_key_values` (if active)
-        (
-            cur_len,
-            running_sequences,
-            running_scores,
-            running_beam_indices,
-            sequences,
-            scores,
-            beam_indices,
-            is_sent_finished,
-            model_kwargs,
-        ) = beam_search_body_fn(
-            cur_len,
-            running_sequences,
-            running_scores,
-            running_beam_indices,
-            sequences,
-            scores,
-            beam_indices,
-            is_sent_finished,
-            model_kwargs,
-        )
-
-        # 2-to-n generation steps can then be run in autoregressive fashion (only in case 1st generation step does
-        # NOT yield EOS token though)
-        maximum_iterations = max_length - cur_len
-        (
-            cur_len,
-            running_sequences,
-            running_scores,
-            running_beam_indices,
-            sequences,
-            scores,
-            beam_indices,
-            is_sent_finished,
-            _,
-        ) = tf.while_loop(
-            beam_search_cond_fn,
-            beam_search_body_fn,
-            (
+            # @tf.function
+            def beam_search_body_fn(
+                tt,
                 cur_len,
                 running_sequences,
                 running_scores,
@@ -2611,63 +2378,352 @@ class TFGenerationMixin:
                 beam_indices,
                 is_sent_finished,
                 model_kwargs,
-            ),
-            maximum_iterations=maximum_iterations,
-        )
+            ):
+                """
+                Beam Search iterative update function -- each iteration adds a new token and updates the best sequences
+                seen so far
+                """
+                # 1. Forward current tokens
+                mk = model_kwargs.copy()
+                print("111111")
+                if mk.get("past_key_values") is None or needs_full_input:
+                    input_ids = running_sequences[:, :, :cur_len]
+                else:
+                    input_ids = tf.expand_dims(running_sequences[:, :, cur_len - 1], -1)
+                model_inputs = self.prepare_inputs_for_generation(
+                    flatten_beam_dim(input_ids), use_cache=use_cache, **mk
+                )
+                model_outputs = self(
+                    **model_inputs,
+                    return_dict=True,
+                    output_attentions=output_attentions,
+                    output_hidden_states=output_hidden_states,
+                )
+                logits = unflatten_beam_dim(model_outputs.logits[:, -1], num_beams)
 
-        # 6. prepare outputs
-        # Account for the edge-case where there are no finished sequences for a particular batch item. If so, return
-        # running sequences for that batch item.
-        none_finished = tf.math.reduce_any(is_sent_finished, axis=1)
-        sequences = tf.where(none_finished[:, None, None], sequences, running_sequences)
-        beam_indices = tf.where(none_finished[:, None, None], beam_indices, running_beam_indices)
+                # 2. Compute log probs
+                # get log probabilities from logits, process logits with processors (*e.g.* min_length, ...), and
+                # add new logprobs to existing running logprobs scores.
+                log_probs = tf.nn.log_softmax(logits)
+                log_probs = logits_processor(flatten_beam_dim(running_sequences), flatten_beam_dim(log_probs), cur_len)
+                log_probs = unflatten_beam_dim(log_probs, num_beams)
+                log_probs_processed = log_probs
+                log_probs = log_probs + tf.expand_dims(running_scores, axis=2)
+                if do_sample:
+                    # Note: logits warpers are intentionally applied after adding running beam scores. On some logits
+                    # warpers (like top_p) this is indiferent, but on others (like temperature) it is not. For reference,
+                    # see https://github.com/huggingface/transformers/pull/5420#discussion_r449779867
+                    log_probs = logits_warper(flatten_beam_dim(running_sequences), flatten_beam_dim(log_probs), cur_len)
+                    log_probs = unflatten_beam_dim(log_probs, num_beams)
+                vocab_size = log_probs.shape[2]
+                log_probs = tf.reshape(log_probs, (batch_size, num_beams * vocab_size))
+
+                # Store scores, attentions and hidden_states when required
+                if not use_xla and return_dict_in_generate:
+                    if output_scores:
+                        all_scores.append(
+                            logits_warper(
+                                flatten_beam_dim(running_sequences), flatten_beam_dim(log_probs_processed), cur_len
+                            )
+                        )
+                    if output_attentions and self.config.is_encoder_decoder:
+                        decoder_attentions.append(model_outputs.decoder_attentions)
+                    elif output_attentions and not self.config.is_encoder_decoder:
+                        decoder_attentions.append(model_outputs.attentions)
+                        if self.config.is_encoder_decoder:
+                            cross_attentions.append(model_outputs.cross_attentions)
+
+                    if output_hidden_states and self.config.is_encoder_decoder:
+                        decoder_hidden_states.append(model_outputs.decoder_hidden_states)
+                    elif output_hidden_states and self.config.is_encoder_decoder:
+                        decoder_hidden_states.append(model_outputs.hidden_states)
+
+                # 3. Retrieve top-K
+                # Each item in batch has num_beams * vocab_size candidate sequences. For each item, get the top 2*k
+                # candidates with the highest log-probabilities. We gather the top 2*K beams here so that even if the
+                # best K sequences reach EOS simultaneously, we have another K sequences remaining to continue the live
+                # beam search.
+                # Gather the top 2*K scores from _all_ beams.
+                # Gather 2*k top beams.
+                # Recover the beam index by floor division.
+                # Recover token id by modulo division and expand Id array for broadcasting.
+                # Update sequences for the 2*K top-k new sequences.
+                beams_to_keep = 2 * num_beams
+                if do_sample:
+                    topk_indices = sample_without_replacement(log_probs, beams_to_keep)
+                    topk_log_probs = tf.gather(log_probs, topk_indices, axis=1, batch_dims=1)
+                else:
+                    topk_log_probs, topk_indices = tf.math.top_k(log_probs, k=beams_to_keep)
+                topk_current_beam_indices = topk_indices // vocab_size
+                topk_running_beam_indices = self._gather_beams(running_beam_indices, topk_current_beam_indices)
+                topk_running_sequences = self._gather_beams(running_sequences, topk_current_beam_indices)
+                topk_ids = topk_indices % vocab_size
+
+                # writes the new token
+                indices_batch = tf.repeat(tf.range(batch_size), [beams_to_keep])
+                indices_beam = tf.tile(tf.range(beams_to_keep), [batch_size])
+                update_indices = tf.stack(
+                    [indices_batch, indices_beam, tf.broadcast_to(cur_len, [batch_size * beams_to_keep])], axis=-1
+                )
+                topk_sequences = tf.tensor_scatter_nd_update(
+                    tensor=topk_running_sequences,
+                    indices=update_indices,
+                    updates=tf.reshape(topk_ids, [batch_size * beams_to_keep]),
+                )
 
-        # Apply the length penalty so that running scores match the finalized scores if they are used
-        running_scores = running_scores / (tf.cast(cur_len, dtype=tf.float32) ** length_penalty)
-        scores = tf.where(none_finished[:, None], scores, running_scores)
+                # we want to store the beam indices with batch information -> real beam index = beam index % num beams
+                batch_modified_indices = topk_current_beam_indices + tf.broadcast_to(
+                    tf.expand_dims(tf.range(batch_size) * num_beams, axis=1), topk_current_beam_indices.shape
+                )
+                topk_beam_indices = tf.tensor_scatter_nd_update(
+                    tensor=topk_running_beam_indices,
+                    indices=update_indices,
+                    updates=tf.reshape(batch_modified_indices, [batch_size * beams_to_keep]),
+                )
 
-        # Take best beams for each batch (the score is sorted in descending order)
-        sequences = flatten_beam_dim(sequences[:, :num_return_sequences, :])
-        scores = flatten_beam_dim(scores[:, :num_return_sequences])
-        beam_indices = flatten_beam_dim(beam_indices[:, :num_return_sequences, :])
+                # 4. Check which sequences have ended
+                # Update current sequences: Did the top `num_beams` sequences reach an end marker?
+                # To prevent these just finished sequences from being added to the current sequences
+                # set of active beam search sequences, set their log probs to a very large negative value.
+                if eos_token_id is None:
+                    eos_in_next_token = tf.zeros(topk_sequences[:, :, cur_len].shape, dtype=tf.bool)
+                else:
+                    eos_in_next_token = tf.math.reduce_any(
+                        tf.equal(
+                            tf.broadcast_to(
+                                topk_sequences[:, :, cur_len], [len(eos_token_id)] + topk_sequences[:, :, cur_len].shape
+                            ),
+                            tf.expand_dims(tf.expand_dims(eos_token_id, -1), -1),
+                        ),
+                        axis=0,
+                    )
+                did_topk_just_finished = eos_in_next_token & tf.broadcast_to(
+                    tf.concat((tf.ones((num_beams), dtype=tf.bool), tf.zeros((num_beams), dtype=tf.bool)), axis=0),
+                    shape_list(eos_in_next_token),
+                )
 
-        if not use_xla:
-            # Cut for backward compatibility
-            sequences = sequences[:, :cur_len]
-            beam_indices = beam_indices[:, :cur_len]
+                # non-top `num_beams` eos tokens can't be used to finish a beam, but the others can't be used in the next
+                # running sentences either
+                running_topk_log_probs = topk_log_probs + tf.cast(tf.cast(eos_in_next_token, tf.float32) * -1.0e9, toDtype)
 
-        if return_dict_in_generate:
-            if self.config.is_encoder_decoder:
-                # if model is an encoder-decoder, retrieve encoder attention weights and hidden states
-                encoder_attentions = model_kwargs["encoder_outputs"].get("attentions") if output_attentions else None
-                encoder_hidden_states = (
-                    model_kwargs["encoder_outputs"].get("hidden_states") if output_hidden_states else None
+                # 5. Get running sequences scores for next
+                # Determine the top k beam indices (from top 2*k beams) from log probs and gather top k beams
+                # (from top 2*k beams).
+                next_topk_indices = tf.math.top_k(running_topk_log_probs, k=num_beams)[1]
+                next_running_sequences, next_running_scores, next_running_beam_indices = self._gather_beams(
+                    [topk_sequences, running_topk_log_probs, topk_beam_indices], next_topk_indices
                 )
 
-                output_cls = TFBeamSampleEncoderDecoderOutput if do_sample else TFBeamSearchEncoderDecoderOutput
-                return output_cls(
-                    sequences=sequences,
-                    sequences_scores=scores,
-                    scores=all_scores,
-                    beam_indices=beam_indices,
-                    encoder_attentions=encoder_attentions,
-                    encoder_hidden_states=encoder_hidden_states,
-                    decoder_attentions=decoder_attentions,
-                    cross_attentions=cross_attentions,
-                    decoder_hidden_states=decoder_hidden_states,
+                # 6. Process topk logits
+                # Further process log probs:
+                # - add length penalty
+                # - make sure no scores can be added anymore if beam is full
+                # - make sure still running sequences cannot be chosen as finalized beam
+                topk_log_probs = topk_log_probs / tf.cast(tf.cast(cur_len, dtype=tf.float32) ** length_penalty, toDtype)
+                beams_in_batch_are_full = tf.broadcast_to(
+                    tf.math.reduce_all(is_sent_finished, axis=-1, keepdims=True), shape_list(did_topk_just_finished)
+                ) & (early_stopping is True)
+                add_penalty = ~did_topk_just_finished | beams_in_batch_are_full
+                topk_log_probs += tf.cast(tf.cast(add_penalty, tf.float32) * -1.0e9, toDtype)
+
+                # 7. Get scores, sequences, is sentence finished for next.
+                # Combine sequences, scores, and flags along the beam dimension and compare new finished sequence scores
+                # to existing finished scores and select the best from the new set of beams
+                merged_sequences = tf.concat([sequences, topk_sequences], axis=1)
+                merged_scores = tf.concat([scores, topk_log_probs], axis=1)
+                merged_beams = tf.concat([beam_indices, topk_beam_indices], axis=1)
+                merged_is_sent_finished = tf.concat([is_sent_finished, did_topk_just_finished], axis=1)
+                topk_merged_indices = tf.math.top_k(merged_scores, k=num_beams)[1]
+                next_sequences, next_scores, next_beam_indices, next_is_sent_finished = self._gather_beams(
+                    [merged_sequences, merged_scores, merged_beams, merged_is_sent_finished], topk_merged_indices
                 )
-            else:
-                output_cls = TFBeamSampleDecoderOnlyOutput if do_sample else TFBeamSearchDecoderOnlyOutput
-                return output_cls(
-                    sequences=sequences,
-                    sequences_scores=scores,
-                    scores=all_scores,
-                    beam_indices=beam_indices,
-                    attentions=decoder_attentions,
-                    hidden_states=decoder_hidden_states,
+
+                # 8. Prepare data for the next iteration
+                # Determine the top k beam indices from the original set of all beams. With these, gather the top k
+                # beam-associated caches.
+                cur_len = cur_len + 1
+                if "past_key_values" in model_outputs:
+                    cache = tf.nest.map_structure(
+                        lambda tensor: unflatten_beam_dim(tensor, num_beams, batch_axis=cache_batch_axis),
+                        model_outputs.past_key_values,
+                    )
+                    next_running_indices = self._gather_beams(topk_current_beam_indices, next_topk_indices)
+                    next_cache = self._gather_beams(cache, next_running_indices, batch_axis=cache_batch_axis)
+                    model_outputs["past_key_values"] = tf.nest.map_structure(
+                        lambda tensor: flatten_beam_dim(tensor, batch_axis=cache_batch_axis), next_cache
+                    )
+
+                if use_xla:
+                    next_model_kwargs = self._update_model_kwargs_for_xla_generation(
+                        model_outputs=model_outputs,
+                        model_kwargs=mk,
+                        cur_len=cur_len,
+                        max_length=max_length,
+                        batch_size=(batch_size * num_beams),
+                        is_encoder_decoder=self.config.is_encoder_decoder,
+                        batch_axis=cache_batch_axis,
+                    )
+                else:
+                    next_model_kwargs = self._update_model_kwargs_for_generation(
+                        model_outputs, mk, is_encoder_decoder=self.config.is_encoder_decoder
+                    )
+
+                    # if we don't cache past_key_values key values we need the whole input
+                    if mk.get("past_key_values", None) is None:
+                        # let's throw out `past_key_values` since we don't want `None` tensors
+                        mk.pop("past_key_values", None)
+
+                return (
+                    tt,
+                    cur_len,
+                    next_running_sequences,
+                    next_running_scores,
+                    next_running_beam_indices,
+                    next_sequences,
+                    next_scores,
+                    next_beam_indices,
+                    next_is_sent_finished,
+                    next_model_kwargs,
                 )
-        else:
-            return sequences
+
+                # 5. run generation
+                # 1st generation step has to be run before to initialize `past_key_values` (if active)
+            
+            
+            tt = tf.timestamp()
+            @tf.function(reduce_retracing=True)
+            def gen1():
+                return beam_search_body_fn(
+                    tt,
+                    cur_len,
+                    running_sequences,
+                    running_scores,
+                    running_beam_indices,
+                    sequences,
+                    scores,
+                    beam_indices,
+                    is_sent_finished,
+                    model_kwargs,
+                )
+
+            (
+                    tt,
+                    cur_len,
+                    running_sequences,
+                    running_scores,
+                    running_beam_indices,
+                    sequences,
+                    scores,
+                    beam_indices,
+                    is_sent_finished,
+                    model_kwargs,
+                ) = gen1()
+
+            # 2-to-n generation steps can then be run in autoregressive fashion (only in case 1st generation step does
+            # NOT yield EOS token though)
+            with tf.control_dependencies([cur_len]):
+                b = tf.timestamp()
+                latency_list.append(b)
+
+                with tf.control_dependencies([b]):
+                    @tf.function(reduce_retracing=True)
+                    def gen2():
+                        maximum_iterations = max_length - cur_len
+                        return tf.while_loop(
+                            beam_search_cond_fn,
+                            beam_search_body_fn,
+                            (
+                                tt,
+                                cur_len,
+                                running_sequences,
+                                running_scores,
+                                running_beam_indices,
+                                sequences,
+                                scores,
+                                beam_indices,
+                                is_sent_finished,
+                                model_kwargs,
+                            ),
+                            maximum_iterations=maximum_iterations,
+                        )            
+        
+                    (
+                        tt,
+                        cur_len,
+                        running_sequences,
+                        running_scores,
+                        running_beam_indices,
+                        sequences,
+                        scores,
+                        beam_indices,
+                        is_sent_finished,
+                        _,
+                    ) = gen2()
+
+                    with tf.control_dependencies([sequences]):
+                        c = tf.timestamp()
+                        latency_list.append(c)
+
+                        with tf.control_dependencies([c]):
+
+                            # 6. prepare outputs
+                            # Account for the edge-case where there are no finished sequences for a particular batch item. If so, return
+                            # running sequences for that batch item.
+                            none_finished = tf.math.reduce_any(is_sent_finished, axis=1)
+                            sequences = tf.where(none_finished[:, None, None], sequences, running_sequences)
+                            beam_indices = tf.where(none_finished[:, None, None], beam_indices, running_beam_indices)
+
+                            # Apply the length penalty so that running scores match the finalized scores if they are used
+                            running_scores = running_scores / tf.cast((tf.cast(cur_len, dtype=tf.float32) ** length_penalty), toDtype)
+                            scores = tf.where(none_finished[:, None], scores, running_scores)
+
+                            # Take best beams for each batch (the score is sorted in descending order)
+                            sequences = flatten_beam_dim(sequences[:, :num_return_sequences, :])
+                            scores = flatten_beam_dim(scores[:, :num_return_sequences])
+                            beam_indices = flatten_beam_dim(beam_indices[:, :num_return_sequences, :])
+
+                            if not use_xla:
+                                # Cut for backward compatibility
+                                sequences = sequences[:, :cur_len]
+                                beam_indices = beam_indices[:, :cur_len]
+
+                            if return_dict_in_generate:
+                                if self.config.is_encoder_decoder:
+                                    # if model is an encoder-decoder, retrieve encoder attention weights and hidden states
+                                    encoder_attentions = model_kwargs["encoder_outputs"].get("attentions") if output_attentions else None
+                                    encoder_hidden_states = (
+                                        model_kwargs["encoder_outputs"].get("hidden_states") if output_hidden_states else None
+                                    )
+
+                                    output_cls = TFBeamSampleEncoderDecoderOutput if do_sample else TFBeamSearchEncoderDecoderOutput
+                                    output_result = output_cls(
+                                        sequences=sequences,
+                                        sequences_scores=scores,
+                                        scores=all_scores,
+                                        beam_indices=beam_indices,
+                                        encoder_attentions=encoder_attentions,
+                                        encoder_hidden_states=encoder_hidden_states,
+                                        decoder_attentions=decoder_attentions,
+                                        cross_attentions=cross_attentions,
+                                        decoder_hidden_states=decoder_hidden_states,
+                                    )
+                                else:
+                                    output_cls = TFBeamSampleDecoderOnlyOutput if do_sample else TFBeamSearchDecoderOnlyOutput
+                                    output_result = output_cls(
+                                        sequences=sequences,
+                                        sequences_scores=scores,
+                                        scores=all_scores,
+                                        beam_indices=beam_indices,
+                                        attentions=decoder_attentions,
+                                        hidden_states=decoder_hidden_states,
+                                    )
+                            else:
+                                output_result = sequences
+
+                            if self.token_latency is not None:
+                                return (output_result, latency_list)
+                            else:
+                                return output_result
 
     def contrastive_search(
         self,
