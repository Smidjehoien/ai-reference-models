diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index cf71499b0..3a24cf17a 100755
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -336,7 +336,7 @@ class Trainer:
         self.hp_name = None
         self.deepspeed = None
         self.is_in_train = False
-
+        self.fp16_scaler = None
         # memory metrics - must set up as early as possible
         self._memory_tracker = TrainerMemoryTracker(self.args.skip_memory_metrics)
         self._memory_tracker.start()
@@ -606,7 +606,7 @@ class Trainer:
                         "but SageMaker Model Parallelism < 1.10 does not support FP16 in trainer."
                     )
 
-        if args.fp16 or args.bf16:
+        if args.fp16 or args.bf16 or args.fp16_cpu:
             if args.half_precision_backend == "auto":
                 if args.device == torch.device("cpu"):
                     if args.fp16:
@@ -621,7 +621,7 @@ class Trainer:
             logger.info(f"Using {args.half_precision_backend} half precision backend")
 
         self.do_grad_scaling = False
-        if (args.fp16 or args.bf16) and not (args.deepspeed or is_sagemaker_mp_enabled()):
+        if (args.fp16 or args.bf16 or args.fp16_cpu) and not (args.deepspeed or is_sagemaker_mp_enabled()):
             # deepspeed and SageMaker Model Parallel manage their own half precision
             if args.half_precision_backend == "cuda_amp":
                 self.use_cuda_amp = True
@@ -645,7 +645,7 @@ class Trainer:
                         self.scaler = torch.cuda.amp.GradScaler()
             elif args.half_precision_backend == "cpu_amp":
                 self.use_cpu_amp = True
-                self.amp_dtype = torch.bfloat16
+                self.amp_dtype = torch.bfloat16 if not args.fp16_cpu else torch.half
             else:
                 if not is_apex_available():
                     raise ImportError(
@@ -1377,9 +1377,13 @@ class Trainer:
         else:
             if not model.training:
                 model.train()
-            model, self.optimizer = ipex.optimize(
-                model, dtype=dtype, optimizer=self.optimizer, inplace=True, level="O1"
-            )
+            if self.args.fp16_cpu:
+                self.fp16_scaler = torch.cpu.amp.GradScaler()
+                model, self.optimizer = ipex.optimize(model, optimizer=self.optimizer, dtype=torch.half, level='O0', weights_prepack=True, inplace=True)
+            else:
+                model, self.optimizer = ipex.optimize(
+                    model, dtype=dtype, optimizer=self.optimizer, inplace=True, level="O1"
+                )
 
         return model
 
@@ -1993,7 +1997,11 @@ class Trainer:
                         scale_after = self.scaler.get_scale()
                         optimizer_was_run = scale_before <= scale_after
                     else:
-                        self.optimizer.step()
+                        if self.args.fp16_cpu:
+                            self.fp16_scaler.step(self.optimizer)
+                            self.fp16_scaler.update()
+                        else:
+                            self.optimizer.step()
 
                     if optimizer_was_run and not self.deepspeed:
                         self.lr_scheduler.step()
@@ -2714,7 +2722,10 @@ class Trainer:
             # loss gets scaled under gradient_accumulation_steps in deepspeed
             loss = self.deepspeed.backward(loss)
         else:
-            loss.backward()
+            if self.args.fp16_cpu:
+                self.fp16_scaler.scale(loss).backward()
+            else:
+                loss.backward()
 
         return loss.detach()
 
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index 088eb06b7..573861eaa 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -781,6 +781,10 @@ class TrainingArguments:
         default=False,
         metadata={"help": "Whether to use fp16 (mixed) precision instead of 32-bit"},
     )
+    fp16_cpu: bool = field(
+        default=False,
+        metadata={"help": "Whether to use fp16 (mixed) precision instead of 32-bit on cpu using IPEX"},
+    )
     fp16_opt_level: str = field(
         default="O1",
         metadata={
