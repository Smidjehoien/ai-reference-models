diff --git a/src/transformers/activations.py b/src/transformers/activations.py
index 587dc2e59..b4e331e28 100644
--- a/src/transformers/activations.py
+++ b/src/transformers/activations.py
@@ -53,8 +53,7 @@ class NewGELUActivation(nn.Module):
     """
 
     def forward(self, input: Tensor) -> Tensor:
-        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
-
+        return nn.functional.gelu(input, approximate='tanh') 
 
 class GELUActivation(nn.Module):
     """
diff --git a/src/transformers/generation/utils.py b/src/transformers/generation/utils.py
index ae12ae293..48fa668d7 100644
--- a/src/transformers/generation/utils.py
+++ b/src/transformers/generation/utils.py
@@ -16,6 +16,8 @@
 
 import copy
 import inspect
+import re
+import time
 import warnings
 from dataclasses import dataclass
 from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union
@@ -700,6 +702,9 @@ class GenerationMixin:
 
     def _extract_past_from_model_output(self, outputs: ModelOutput, standardize_cache_format: bool = False):
         past_key_values = None
+        # To use torch.jit.trace, the output is no longer a Dict. outputs[1] corresponds to "past_key_values"
+        if self.jit == True:
+            past_key_values = outputs[1]
         if "past_key_values" in outputs:
             past_key_values = outputs.past_key_values
         elif "mems" in outputs:
@@ -1208,6 +1213,11 @@ class GenerationMixin:
 
         # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call
         self._validate_model_class()
+        self.jit = kwargs.pop("jit", False)
+        self.quantized_model_path = kwargs.pop("quantized_model_path", None)
+        self.ipex_int8 = kwargs.pop("ipex_int8", False)
+        self.tp_number = kwargs.pop("TP_number", 1)
+        self.token_latency = kwargs.pop("token_latency", None)
 
         # priority: `generation_config` argument > `model.generation_config` (the default generation config)
         if generation_config is None:
@@ -2186,6 +2196,7 @@ class GenerationMixin:
         ["It might be possible to get a better understanding of the nature of the problem, but it's not"]
         ```"""
         # init values
+        latency_list = []
         logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
         stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
         if max_length is not None:
@@ -2231,6 +2242,7 @@ class GenerationMixin:
 
         this_peer_finished = False  # used by synced_gpus only
         while True:
+            tic = time.time()
             if synced_gpus:
                 # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.
                 # The following logic allows an early break if all peers finished generating their sequence
@@ -2243,19 +2255,92 @@ class GenerationMixin:
 
             # prepare model inputs
             model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
-
-            # forward pass to get next token
-            outputs = self(
-                **model_inputs,
-                return_dict=True,
-                output_attentions=output_attentions,
-                output_hidden_states=output_hidden_states,
-            )
-
-            if synced_gpus and this_peer_finished:
-                continue  # don't waste resources running the code we don't need
-
-            next_token_logits = outputs.logits[:, -1, :]
+            if re.search("GPTJ", self.config.architectures[0]) or re.search("llama", self.config.architectures[0], re.IGNORECASE) or re.search("bloom", self.config.architectures[0], re.IGNORECASE):
+                if self.jit == False:
+                    outputs = self(
+                        **model_inputs,
+                        return_dict=True,
+                        output_attentions=output_attentions,
+                        output_hidden_states=output_hidden_states,
+                        )
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    next_token_logits = outputs.logits[:, -1, :]
+                else:
+                    first_token = False
+                    input_bs = input_ids.size()[0]
+                    if model_inputs["past_key_values"] is None:
+                        first_token = True
+                    if first_token:
+                        seq_len = input_ids.size()[1]
+                        if re.search("GPTJ", self.config.architectures[0]):
+                            beam_idx_tmp=torch.zeros(int(input_bs), dtype=torch.int)
+                            model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.n_embd/self.config.n_head)]), torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.n_embd/self.config.n_head)]), beam_idx_tmp) for i in range(self.config.n_layer)])
+                            model_inputs["position_ids"] = model_inputs["position_ids"][:1,:]
+                        elif re.search("llama", self.config.architectures[0], re.IGNORECASE):
+                            model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.num_attention_heads/self.tp_number),1,int(self.config.hidden_size/self.config.num_attention_heads)]), torch.zeros([1,int(self.config.num_attention_heads/self.tp_number),1,int(self.config.hidden_size/self.config.num_attention_heads)])) for i in range(self.config.num_hidden_layers)])
+                            model_inputs["position_ids"] = model_inputs["position_ids"][:1,:]
+                        elif re.search("bloom", self.config.architectures[0], re.IGNORECASE):
+                            model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.hidden_size/self.config.n_head)]), torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.hidden_size/self.config.n_head)])) for i in range(self.config.n_layer)])
+                        model_inputs["attention_mask"] = model_inputs["attention_mask"][:1,:]
+                        model_inputs["input_ids"] = model_inputs["input_ids"][:1,:]
+                        model_inputs["attention_mask"] = torch.cat([torch.zeros(1, 1), model_inputs["attention_mask"]], dim=-1)
+                    else:
+                        model_inputs["attention_mask"] = torch.cat([torch.zeros(input_bs, 1), model_inputs["attention_mask"]], dim=-1)
+                    model_inputs.pop("use_cache", None)
+                    model_inputs.pop("token_type_ids", None)
+
+                    if not hasattr(self, "trace_graph") and self.jit and self.ipex_int8:
+                        print("load_int8_model")
+                        self_jit = torch.jit.load(self.quantized_model_path)
+                        self_jit = torch.jit.freeze(self_jit.eval())
+                        setattr(self, "trace_graph", self_jit)
+                    if not hasattr(self,"trace_graph") and self.jit and not self.ipex_int8:
+                        if hasattr(self, "forward"):
+                            sig = inspect.signature(self.forward)
+                        else:
+                            sig = inspect.signature(self.call)
+                        example_inputs = tuple(model_inputs[key] for key in sig.parameters
+                            if model_inputs.get(key, None) is not None and not isinstance(model_inputs.get(key, None), bool))
+                        self_jit = torch.jit.trace(self, example_inputs, strict=False)
+                        self_jit = torch.jit.freeze(self_jit.eval())
+                        setattr(self, "trace_graph", self_jit)
+                    outputs = self.trace_graph(**model_inputs)
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    if first_token:
+                        outputs = list(outputs)
+                        outputs[0] = outputs[0].expand(input_bs, -1, -1)
+                        past_key_values = []
+                        for key, value in outputs[1]:
+                            key_dim = key.dim()
+                            value_dim = value.dim()
+                            key = key.expand(input_bs, -1, -1, -1).contiguous()
+                            value = value.expand(input_bs, -1, -1, -1).contiguous()
+                            if key_dim == 3:
+                                key = key.view(key.size(1) * key.size(0), key.size(2), key.size(3))
+                            if value_dim == 3:
+                                value = value.view(value.size(1) * value.size(0), value.size(2), value.size(3))
+                            past_key_values.append(tuple([key, value]))
+                        outputs[1] = tuple(past_key_values)
+                        outputs = tuple(outputs)
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    next_token_logits = outputs[0][:, -1, :]
+            else:
+                outputs = self(
+                    **model_inputs,
+                    return_dict=True,
+                    output_attentions=output_attentions,
+                    output_hidden_states=output_hidden_states,
+                    )
+                if synced_gpus and this_peer_finished:
+                    cur_len = cur_len + 1
+                    continue  # don't waste resources running the code we don't need
+                next_token_logits = outputs.logits[:, -1, :]
 
             # pre-process distribution
             next_tokens_scores = logits_processor(input_ids, next_token_logits)
@@ -2302,6 +2387,7 @@ class GenerationMixin:
                 )
 
             # stop when each sentence is finished, or if we exceed the maximum length
+            latency_list.append(time.time() - tic)
             if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):
                 if not synced_gpus:
                     break
@@ -2313,7 +2399,7 @@ class GenerationMixin:
 
         if return_dict_in_generate:
             if self.config.is_encoder_decoder:
-                return GreedySearchEncoderDecoderOutput(
+                output_result = GreedySearchEncoderDecoderOutput(
                     sequences=input_ids,
                     scores=scores,
                     encoder_attentions=encoder_attentions,
@@ -2323,14 +2409,19 @@ class GenerationMixin:
                     decoder_hidden_states=decoder_hidden_states,
                 )
             else:
-                return GreedySearchDecoderOnlyOutput(
+                output_result = GreedySearchDecoderOnlyOutput(
                     sequences=input_ids,
                     scores=scores,
                     attentions=decoder_attentions,
                     hidden_states=decoder_hidden_states,
                 )
         else:
-            return input_ids
+            output_result = input_ids
+
+        if self.token_latency is not None:
+            return (output_result, latency_list)
+        else:
+            return output_result
 
     def sample(
         self,
@@ -2733,6 +2824,7 @@ class GenerationMixin:
         ['Wie alt bist du?']
         ```"""
         # init values
+        latency_list = []
         logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()
         stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()
         if max_length is not None:
@@ -2795,6 +2887,7 @@ class GenerationMixin:
 
         this_peer_finished = False  # used by synced_gpus only
         while True:
+            tic = time.time()
             if synced_gpus:
                 # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.
                 # The following logic allows an early break if all peers finished generating their sequence
@@ -2806,19 +2899,131 @@ class GenerationMixin:
                     break
 
             model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
-
-            outputs = self(
-                **model_inputs,
-                return_dict=True,
-                output_attentions=output_attentions,
-                output_hidden_states=output_hidden_states,
-            )
-
-            if synced_gpus and this_peer_finished:
-                cur_len = cur_len + 1
-                continue  # don't waste resources running the code we don't need
-
-            next_token_logits = outputs.logits[:, -1, :]
+            if re.search("GPTJ", self.config.architectures[0]) or re.search("llama", self.config.architectures[0], re.IGNORECASE):
+                if self.jit == False:
+                    outputs = self(
+                        **model_inputs,
+                        return_dict=True,
+                        output_attentions=output_attentions,
+                        output_hidden_states=output_hidden_states,
+                        )
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    next_token_logits = outputs.logits[:, -1, :]
+                else:
+                    first_token = False
+                    input_bs = input_ids.size()[0]
+                    if model_inputs["past_key_values"] is None:
+                        first_token = True
+                    if first_token:
+                        seq_len = input_ids.size()[1]
+                        if re.search("GPTJ", self.config.architectures[0]):
+                            beam_idx_tmp=torch.zeros(int(batch_size * num_beams), dtype=torch.int)
+                            model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.n_embd/self.config.n_head)]), torch.zeros([1,int(self.config.n_head/self.tp_number),1,int(self.config.n_embd/self.config.n_head)]), beam_idx_tmp) for i in range(self.config.n_layer)])
+                        elif re.search("llama", self.config.architectures[0], re.IGNORECASE):
+                            model_inputs["past_key_values"] = tuple([(torch.zeros([1,int(self.config.num_attention_heads/self.tp_number),1,int(self.config.hidden_size/self.config.num_attention_heads)]), torch.zeros([1,int(self.config.num_attention_heads/self.tp_number),1,int(self.config.hidden_size/self.config.num_attention_heads)])) for i in range(self.config.num_hidden_layers)])
+
+                        model_inputs["attention_mask"] = model_inputs["attention_mask"][:1,:]
+                        model_inputs["input_ids"] = model_inputs["input_ids"][:1,:]
+                        model_inputs["position_ids"] = model_inputs["position_ids"][:1,:]
+                        model_inputs["attention_mask"] = torch.cat([torch.zeros(1, 1), model_inputs["attention_mask"]], dim=-1)
+                    else:
+                        model_inputs["attention_mask"] = torch.cat([torch.zeros(input_bs, 1), model_inputs["attention_mask"]], dim=-1)
+                    model_inputs.pop("use_cache", None)
+                    model_inputs.pop("token_type_ids", None)
+
+                    if not hasattr(self, "trace_graph") and self.jit and self.ipex_int8:
+                        print("load_int8_model")
+                        self_jit = torch.jit.load(self.quantized_model_path)
+                        self_jit = torch.jit.freeze(self_jit.eval())
+                        setattr(self, "trace_graph", self_jit)
+                    if not hasattr(self,"trace_graph") and self.jit and not self.ipex_int8:
+                        if hasattr(self, "forward"):
+                            sig = inspect.signature(self.forward)
+                        else:
+                            sig = inspect.signature(self.call)
+                        example_inputs = tuple(model_inputs[key] for key in sig.parameters
+                            if model_inputs.get(key, None) is not None and not isinstance(model_inputs.get(key, None), bool))
+                        self_jit = torch.jit.trace(self, example_inputs, strict=False)
+                        self_jit = torch.jit.freeze(self_jit.eval())
+                        setattr(self, "trace_graph", self_jit)
+                    outputs = self.trace_graph(**model_inputs)
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    if first_token:
+                        outputs = list(outputs)
+                        outputs[0] = outputs[0].expand(input_bs, -1, -1)
+                        past_key_values = []
+                        for key, value in outputs[1]:
+                            key_dim = key.dim()
+                            value_dim = value.dim()
+                            key = key.expand(input_bs, -1, -1, -1).contiguous()
+                            value = value.expand(input_bs, -1, -1, -1).contiguous()
+                            if key_dim == 3:
+                                key = key.view(key.size(1) * key.size(0), key.size(2), key.size(3))
+                            if value_dim == 3:
+                                value = value.view(value.size(1) * value.size(0), value.size(2), value.size(3))
+                            past_key_values.append(tuple([key, value]))
+                        outputs[1] = tuple(past_key_values)
+                        outputs = tuple(outputs)
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    next_token_logits = outputs[0][:, -1, :]
+            else:
+                if model_inputs["past_key_values"] is None or self.jit == False:
+                    if re.search("T5", self.config.architectures[0]):
+                        first_token = False
+                    else:
+                        first_token = model_inputs["input_ids"].size()[1] != 1
+                    if first_token: 
+                        input_bs = input_ids.size()[0]
+                        seq_len = input_ids.size()[1]
+                        model_inputs["attention_mask"] = model_inputs["attention_mask"][:1,:]
+                        model_inputs["input_ids"] = model_inputs["input_ids"][:1,:]
+                    outputs = self(
+                        **model_inputs,
+                        return_dict=True,
+                        output_attentions=output_attentions,
+                        output_hidden_states=output_hidden_states,
+                    )
+                    if first_token: 
+                        outputs.logits = outputs.logits.expand(input_bs, seq_len, -1)
+                        past_key_values = []
+                        for key, value in outputs["past_key_values"]:
+                            key_dim = key.dim()
+                            value_dim = value.dim()
+                            key = key.expand(input_bs, -1, -1, -1).contiguous()
+                            value = value.expand(input_bs, -1, -1, -1).contiguous()
+                            if key_dim == 3:
+                                key = key.view(key.size(1) * key.size(0), key.size(2), key.size(3))
+                            if value_dim == 3:
+                                value = value.view(value.size(1) * value.size(0), value.size(2), value.size(3))
+                            past_key_values.append(tuple([key, value]))
+                        outputs.past_key_values = tuple(past_key_values)
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need               
+                    next_token_logits = outputs.logits[:, -1, :]          
+                else:
+                    if hasattr(self, "forward"):
+                        sig = inspect.signature(self.forward)
+                    else:
+                        sig = inspect.signature(self.call)
+                    example_inputs = tuple(model_inputs[key] for key in sig.parameters
+                        if model_inputs.get(key, None) is not None and not isinstance(model_inputs.get(key, None), bool))
+                    if not hasattr(self,"trace_graph") and self.jit and not self.ipex_int8:
+                        self_jit = torch.jit.trace(self, example_inputs, strict=False)
+                        self_jit = torch.jit.freeze(self_jit.eval())
+                        setattr(self, "trace_graph", self_jit)
+
+                    outputs = self.trace_graph(*example_inputs)
+                    if synced_gpus and this_peer_finished:
+                        cur_len = cur_len + 1
+                        continue  # don't waste resources running the code we don't need
+                    next_token_logits = outputs[0][:, -1, :]               
             # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`
             # cannot be generated both before and after the `nn.functional.log_softmax` operation.
             next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)
@@ -2887,6 +3092,7 @@ class GenerationMixin:
 
             # increase cur_len
             cur_len = cur_len + 1
+            latency_list.append(time.time() - tic)
 
             if beam_scorer.is_done or stopping_criteria(input_ids, scores):
                 if not synced_gpus:
@@ -2910,7 +3116,7 @@ class GenerationMixin:
                 sequence_outputs["sequence_scores"] = None
 
             if self.config.is_encoder_decoder:
-                return BeamSearchEncoderDecoderOutput(
+                output_result = BeamSearchEncoderDecoderOutput(
                     sequences=sequence_outputs["sequences"],
                     sequences_scores=sequence_outputs["sequence_scores"],
                     scores=scores,
@@ -2922,7 +3128,7 @@ class GenerationMixin:
                     decoder_hidden_states=decoder_hidden_states,
                 )
             else:
-                return BeamSearchDecoderOnlyOutput(
+                output_result = BeamSearchDecoderOnlyOutput(
                     sequences=sequence_outputs["sequences"],
                     sequences_scores=sequence_outputs["sequence_scores"],
                     scores=scores,
@@ -2931,7 +3137,12 @@ class GenerationMixin:
                     hidden_states=decoder_hidden_states,
                 )
         else:
-            return sequence_outputs["sequences"]
+            output_result = sequence_outputs["sequences"]
+        # result
+        if self.token_latency is not None:
+            return (output_result, latency_list)
+        else:
+            return output_result
 
     def beam_sample(
         self,
diff --git a/src/transformers/models/gptj/modeling_gptj.py b/src/transformers/models/gptj/modeling_gptj.py
index 1051a298d..e638b75d2 100644
--- a/src/transformers/models/gptj/modeling_gptj.py
+++ b/src/transformers/models/gptj/modeling_gptj.py
@@ -210,33 +210,21 @@ class GPTJAttention(nn.Module):
         query = self._split_heads(query, self.num_attention_heads, self.head_dim, True)
         key = self._split_heads(key, self.num_attention_heads, self.head_dim, True)
         value = self._split_heads(value, self.num_attention_heads, self.head_dim, False)
-
-        if is_torch_fx_proxy(position_ids):
-            # The logic to conditionally copy to GPU could not be traced, so we do this
-            # every time in the torch.fx case
-            embed_positions = get_embed_positions(self.embed_positions, position_ids)
-        else:
-            embed_positions = self._get_embed_positions(position_ids)
-
-        repeated_position_ids = position_ids.unsqueeze(-1).repeat(1, 1, embed_positions.shape[-1])
-        sincos = torch.gather(embed_positions, 1, repeated_position_ids)
-        sin, cos = torch.split(sincos, sincos.shape[-1] // 2, dim=-1)
-
-        if self.rotary_dim is not None:
-            k_rot = key[:, :, :, : self.rotary_dim]
-            k_pass = key[:, :, :, self.rotary_dim :]
-
-            q_rot = query[:, :, :, : self.rotary_dim]
-            q_pass = query[:, :, :, self.rotary_dim :]
-
-            k_rot = apply_rotary_pos_emb(k_rot, sin, cos)
-            q_rot = apply_rotary_pos_emb(q_rot, sin, cos)
-
-            key = torch.cat([k_rot, k_pass], dim=-1)
-            query = torch.cat([q_rot, q_pass], dim=-1)
-        else:
-            key = apply_rotary_pos_emb(key, sin, cos)
-            query = apply_rotary_pos_emb(query, sin, cos)
+        position_ids = position_ids.contiguous()
+        torch.ops.torch_ipex.rotary_position_embedding(
+            key,
+            self.embed_positions,
+            position_ids,
+            self.num_attention_heads,
+            self.head_dim,
+        )
+        torch.ops.torch_ipex.rotary_position_embedding(
+            query,
+            self.embed_positions,
+            position_ids,
+            self.num_attention_heads,
+            self.head_dim,
+        )
 
         key = key.permute(0, 2, 1, 3)
         query = query.permute(0, 2, 1, 3)
@@ -244,8 +232,23 @@ class GPTJAttention(nn.Module):
         if layer_past is not None:
             past_key = layer_past[0]
             past_value = layer_past[1]
-            key = torch.cat((past_key, key), dim=-2)
-            value = torch.cat((past_value, value), dim=-2)
+            if len(layer_past) == 3:
+                key = key.detach()
+                value = value.detach()
+                beam_idx = layer_past[2]
+                bs, hn, past_len, hs = past_key.shape
+                cur_len = key.shape[2]
+                new_key = torch.empty((bs, hn, past_len+cur_len, hs), dtype=key.dtype)
+                new_value = torch.empty((bs, hn, past_len+cur_len, hs), dtype=value.dtype)
+                for i, idx in enumerate(beam_idx):
+                    torch.cat((past_key[idx], key[i%bs]),dim=-2, out=new_key[i%bs])
+                    torch.cat((past_value[idx], value[i%bs]),dim=-2, out=new_value[i%bs])
+                key = new_key
+                value = new_value
+
+            else:
+                key = torch.cat((past_key, key), dim=-2)
+                value = torch.cat((past_value, value), dim=-2)
 
         if use_cache is True:
             present = (key, value)
@@ -829,10 +832,10 @@ class GPTJForCausalLM(GPTJPreTrainedModel):
     def forward(
         self,
         input_ids: Optional[torch.LongTensor] = None,
-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
         attention_mask: Optional[torch.FloatTensor] = None,
-        token_type_ids: Optional[torch.LongTensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
         position_ids: Optional[torch.LongTensor] = None,
+        token_type_ids: Optional[torch.LongTensor] = None,
         head_mask: Optional[torch.FloatTensor] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
         labels: Optional[torch.LongTensor] = None,
@@ -906,11 +909,14 @@ class GPTJForCausalLM(GPTJPreTrainedModel):
         [`~PretrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct
         beam_idx at every generation step.
         """
-        return tuple(
-            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)
-            for layer_past in past_key_values
-        )
-
+        new_layer_past=[]
+        for layer_past in past_key_values:
+            new_past_state=[]
+            for past_state in layer_past:
+                new_past_state.append(past_state)
+            new_past_state.append(beam_idx.to(past_state.device))
+            new_layer_past.append(tuple(tuple(new_past_state)))
+        return tuple(new_layer_past)
 
 @add_start_docstrings(
     """
diff --git a/src/transformers/models/llama/modeling_llama.py b/src/transformers/models/llama/modeling_llama.py
index 1ae058469..fe29d059b 100755
--- a/src/transformers/models/llama/modeling_llama.py
+++ b/src/transformers/models/llama/modeling_llama.py
@@ -224,7 +224,7 @@ class LlamaAttention(nn.Module):
                 raise ValueError(
                     f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
                 )
-            attn_weights = attn_weights + attention_mask
+            attn_weights = torch.tensor(attn_weights) + torch.tensor(attention_mask)
             attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))
 
         # upcast attention to fp32
@@ -474,7 +474,7 @@ class LlamaModel(LlamaPreTrainedModel):
                 inputs_embeds.device
             )
             combined_attention_mask = (
-                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask
+                expanded_attn_mask if combined_attention_mask is None else torch.tensor(expanded_attn_mask) + torch.tensor(combined_attention_mask)
             )
 
         return combined_attention_mask
@@ -642,8 +642,8 @@ class LlamaForCausalLM(LlamaPreTrainedModel):
         self,
         input_ids: torch.LongTensor = None,
         attention_mask: Optional[torch.Tensor] = None,
-        position_ids: Optional[torch.LongTensor] = None,
         past_key_values: Optional[List[torch.FloatTensor]] = None,
+        position_ids: Optional[torch.LongTensor] = None,
         inputs_embeds: Optional[torch.FloatTensor] = None,
         labels: Optional[torch.LongTensor] = None,
         use_cache: Optional[bool] = None,
