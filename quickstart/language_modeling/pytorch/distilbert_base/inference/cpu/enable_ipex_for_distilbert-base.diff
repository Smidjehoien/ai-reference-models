diff --git a/examples/pytorch/question-answering/run_qa.py b/examples/pytorch/question-answering/run_qa.py
index 1f78503ad..8eb35aa9d 100755
--- a/examples/pytorch/question-answering/run_qa.py
+++ b/examples/pytorch/question-answering/run_qa.py
@@ -298,6 +298,7 @@ def main():
         model_args.config_name if model_args.config_name else model_args.model_name_or_path,
         cache_dir=model_args.cache_dir,
         revision=model_args.model_revision,
+        return_dict=False,
         use_auth_token=True if model_args.use_auth_token else None,
     )
     tokenizer = AutoTokenizer.from_pretrained(
diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py
index a2fb10b9e..001c15fe2 100755
--- a/src/transformers/trainer.py
+++ b/src/transformers/trainer.py
@@ -1027,7 +1027,62 @@ class Trainer:
 
         return model
 
-    def _wrap_model(self, model, training=True):
+    def _wrap_model(self, model, training=True, dataloader=None):
+        if not self.args.do_calibration and not training:
+            model.eval()
+            import torch
+            if self.args.use_ipex:
+                import intel_extension_for_pytorch as ipex
+
+            if self.args.jit_mode:
+                jit_inputs=()
+                for _,batch in enumerate(dataloader):
+                    example_input = batch
+                    for _,label in enumerate(batch):
+                        dumpy_tensor = torch.ones((batch[label].shape), dtype=torch.long)
+                        L1=list(jit_inputs)
+                        L1.append(dumpy_tensor)
+                        jit_inputs=tuple(L1)
+                    break
+
+                if self.args.use_ipex:
+                    if self.args.mix_bf16:
+                        model = ipex.optimize(model, dtype=torch.bfloat16, level="O1")
+                        with torch.cpu.amp.autocast(), torch.no_grad():
+                            model = torch.jit.trace(model, jit_inputs, strict=False)
+                        model = torch.jit.freeze(model)
+                    elif self.args.int8:
+                        conf = ipex.quantization.QuantConf(configure_file=self.args.int8_config)
+                        # convert model to trace model.
+                        if self.args.mix_bf16:
+                            with torch.cpu.amp.autocast():
+                                model = ipex.quantization.convert(model, conf, jit_inputs)
+                        else:
+                            model = ipex.quantization.convert(model, conf, jit_inputs)
+                        # enable fusion path work(need to run two interation)
+                        with torch.no_grad():
+                            y = model(**example_input)
+                            y = model(**example_input)
+                    else:
+                        model = ipex.optimize(model, dtype=torch.float32, level="O1", auto_kernel_selection=self.args.auto_kernel_selection)
+                        with torch.no_grad():
+                            model = torch.jit.trace(model, jit_inputs, strict=False)
+                        if self.args.jit_opt:
+                            model = torch.jit.optimize_for_inference(model)
+                        else:
+                            model = torch.jit.freeze(model)
+                else:
+                    if self.args.int8:
+                        import torch.quantization
+                        model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
+                    with torch.no_grad():
+                        model = torch.jit.trace(model, jit_inputs, strict=False)
+                    if self.args.jit_opt:
+                        model = torch.jit.optimize_for_inference(model)
+                    else:
+                        model = torch.jit.freeze(model)
+
+
         if is_sagemaker_mp_enabled():
             # Wrapping the base model twice in a DistributedModel will raise an error.
             if isinstance(self.model_wrapped, smp.model.DistributedModel):
@@ -2372,6 +2427,38 @@ class Trainer:
 
         return PredictionOutput(predictions=output.predictions, label_ids=output.label_ids, metrics=output.metrics)
 
+    def benchmark_evaluate(self, model, dataloader):
+        steps_per_epoch = len(dataloader)
+        total_steps = (self.args.perf_run_iters + self.args.perf_begin_iter)
+        test_epoches = int(total_steps / steps_per_epoch)
+        print('Evaluating BERT: Steps per Epoch {} total Steps {}'.format(steps_per_epoch, total_steps))
+        i = 0;
+        timeBuff = []
+        import time
+        with tqdm(total=total_steps, desc="Evaluating") as pbar:
+            for epoch in range(test_epoches + 1):
+                for it, batch in enumerate(dataloader):
+                    if epoch * steps_per_epoch + it >= total_steps:
+                        timeBuff = np.asarray(timeBuff)
+                        totalTime = np.sum(timeBuff)
+                        p50 = np.percentile(timeBuff, 50) # return 50th percentile, e.g median.
+                        p90 = np.percentile(timeBuff, 90)
+                        print("#############################")
+                        print("#############################")
+                        print('P50 Latency {:.2f} ms'.format(p50*1000))
+                        print('P90 Latency {:.2f} ms'.format(p90*1000))
+                        print('Throughput: {:.2f} sentences/s'.format(self.args.per_device_eval_batch_size*self.args.perf_run_iters/totalTime))
+                        print("#############################")
+                        break
+                    with torch.no_grad():
+                        start = time.time()
+                        outputs = model(**batch)
+                        end = time.time()
+                        if epoch * steps_per_epoch + it > self.args.perf_begin_iter:
+                            timeBuff.append(end-start)
+                        pbar.update(1)
+
+
     def evaluation_loop(
         self,
         dataloader: DataLoader,
@@ -2401,7 +2488,7 @@ class Trainer:
             self.model_wrapped = deepspeed_engine
             self.deepspeed = deepspeed_engine
 
-        model = self._wrap_model(self.model, training=False)
+        model = self._wrap_model(self.model, training=False, dataloader=dataloader)
 
         # if full fp16 or bf16 eval is wanted and this ``evaluation`` or ``predict`` isn't called
         # while ``train`` is running, cast it to the right dtype first and then put on device
@@ -2444,6 +2531,41 @@ class Trainer:
         # Will be useful when we have an iterable dataset so don't know its length.
 
         observed_num_examples = 0
+
+        if self.args.do_calibration:
+            import intel_extension_for_pytorch as ipex
+            conf = ipex.quantization.QuantConf(qscheme=torch.per_tensor_affine)
+            for step, inputs in enumerate(dataloader):
+                print("calibration step: {}".format(step))
+                # Update the observed num examples
+                observed_batch_size = find_batch_size(inputs)
+                if observed_batch_size is not None:
+                    observed_num_examples += observed_batch_size
+                    # For batch samplers, batch_size is not known by the dataloader in advance.
+                    if batch_size is None:
+                       batch_size = observed_batch_size
+
+                with ipex.quantization.calibrate(conf):
+                    self.prediction_step(self.model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
+                if step == self.args.calibration_iters -1:
+                    conf.save(self.args.int8_config)
+                    exit()
+
+        if args.benchmark:
+            if self.args.use_share_weight:
+                threads = []
+                import threading
+                num_instances = self.args.total_cores // self.args.cores_per_instance
+                for i in range(0, num_instances):
+                     t = threading.Thread(target=self.benchmark_evaluate, args=(model, dataloader))
+                     threads.append(t)
+                     t.start()
+                for t in threads:
+                    t.join()
+            else:
+                self.benchmark_evaluate(model, dataloader)
+            exit()
+
         # Main evaluation loop
         for step, inputs in enumerate(dataloader):
             # Update the observed num examples
diff --git a/src/transformers/training_args.py b/src/transformers/training_args.py
index 6176794ab..56be08614 100644
--- a/src/transformers/training_args.py
+++ b/src/transformers/training_args.py
@@ -756,6 +756,96 @@ class TrainingArguments:
         default="",
         metadata={"help": "Used by the SageMaker launcher to send mp-specific args. Ignored in Trainer"},
     )
+    jit_mode: bool = field(
+        default=False,
+        metadata={
+            "help": "Enable jit mode for inference"
+        },
+    )
+    jit_opt: bool = field(
+        default=False,
+        metadata={
+            "help": "Enable pytorch jit optimize for inference"
+        },
+    )
+    use_ipex: bool = field(
+        default=False,
+        metadata={
+            "help": "Enable ipex for inference"
+        },
+    )
+    mix_bf16: bool = field(
+        default=False,
+        metadata={
+            "help": "Enable ipex bf16 mix-precision"
+        },
+    )
+    int8: bool = field(
+        default=False,
+        metadata={
+            "help": "Enable int8 low-precision inference"
+        },
+    )
+    use_share_weight: bool = field(
+        default=False,
+        metadata={
+            "help": "Enable weight sharing for real time mode inference"
+        },
+    )
+    total_cores: int = field(
+        default=56,
+        metadata={
+            "help": "Total cores one socket for use_share_weight"
+        },
+    )
+    cores_per_instance: int = field(
+        default=4,
+        metadata={
+            "help": "cores per instance for use_share_weight"
+        },
+    )
+    do_calibration: bool = field(
+        default=False,
+        metadata={
+            "help": "Enable calibration process for ipex int8"
+        },
+    )
+    calibration_iters: int = field(
+        default=400,
+        metadata={
+            "help": "The iterations for calibration"
+        },
+    )
+    int8_config:str = field(
+        default="",
+        metadata={
+            "help": "The calibration result for int8 config"
+        }
+    )
+    auto_kernel_selection: bool = field(
+        default=False,
+        metadata={
+            "help": "Enable mkldnn for ipex fp32"
+        },
+    )
+    benchmark: bool = field(
+        default=False,
+        metadata={
+            "help": "doing the customized benchmark process, getting P50, P90, THP"
+        },
+    )
+    perf_run_iters: int = field(
+        default=100,
+        metadata={
+            "help": "The iterations number for benchmark"
+        },
+    )
+    perf_begin_iter: int = field(
+        default=10,
+        metadata={
+            "help": "The iteration to start the benchmark iterations"
+        },
+    )
 
     def __post_init__(self):
         # Handle --use_env option in torch.distributed.launch (local_rank not passed as an arg then).
