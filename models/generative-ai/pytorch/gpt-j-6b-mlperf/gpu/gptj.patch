diff --git a/gptj.py b/gptj.py
new file mode 100644
index 0000000..8f3f91d
--- /dev/null
+++ b/gptj.py
@@ -0,0 +1,249 @@
+import os
+import argparse
+import time
+import numpy as np
+import torch
+import torch.nn as nn
+import quant
+
+
+from cnn_dm_dataset import CNNDAILYMAIL
+from torch.utils.data import DataLoader
+
+from gptq import GPTQ, Observer
+from utils import find_layers, DEV
+
+def get_gptj(model):
+
+    def skip(*args, **kwargs):
+        pass
+
+    torch.nn.init.kaiming_uniform_ = skip
+    torch.nn.init.uniform_ = skip
+    torch.nn.init.normal_ = skip
+    from transformers import GPTJForCausalLM, AutoModelForCausalLM
+    model = GPTJForCausalLM.from_pretrained(model, torch_dtype=torch.bfloat16) # TODO: Change to fp16?? --> Error: "LayerNormKernelImpl" not implemented for 'Half'
+    model.seqlen = 2048
+    return model
+
+
+@torch.no_grad()
+def gptj_sequential(model, dataloader, dev, quantizers=dict()):
+    print('Starting ...')
+
+    use_cache = model.config.use_cache
+    model.config.use_cache = False
+    layers = model.transformer.h
+    model.transformer.wte = model.transformer.wte.to(dev)
+    model.transformer.ln_f = model.transformer.ln_f.to(dev)
+
+    layers[0] = layers[0].to(dev)
+
+    dtype = next(iter(model.parameters())).dtype
+    inps = torch.zeros((args.nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev)
+    cache = {'i': 0, 'attention_mask': None}
+
+    class Catcher(nn.Module):
+
+        def __init__(self, module):
+            super().__init__()
+            self.module = module
+
+        def forward(self, inp, **kwargs):
+            inps[cache['i']] = inp
+            cache['i'] += 1
+            cache['attention_mask'] = kwargs['attention_mask']
+            #position_ids = kwargs['position_ids']
+            #cache['position_ids'] = position_ids #kwargs['position_ids']
+            raise ValueError
+
+    layers[0] = Catcher(layers[0])
+    for batch in dataloader:
+        try:
+            model(batch[0].to(dev))
+        except ValueError:
+            pass
+    layers[0] = layers[0].module
+
+    layers[0] = layers[0].cpu()
+    model.transformer.wte = model.transformer.wte.to(dev)
+
+    model.transformer.ln_f = model.transformer.ln_f.to(dev)
+    if dev != torch.device('cpu'):
+        torch.cuda.empty_cache()
+
+    outs = torch.zeros_like(inps)
+    attention_mask = cache['attention_mask']
+    #position_ids = cache['position_ids']
+
+    print('Ready.')
+
+    observer = Observer()
+    for i in range(len(layers)):
+
+        print(f'Quantizing layer {i+1}/{len(layers)}..')
+        print('+------------------+--------------+------------+-----------+-------+')
+        print('|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |')
+        print('+==================+==============+============+===========+=======+')
+
+        layer = layers[i].to(dev)
+        full = find_layers(layer)
+        sequential = [list(full.keys())]
+
+        for names in sequential:
+            subset = {n: full[n] for n in names}
+            gptq = {}
+            for name in subset:
+                gptq[name] = GPTQ(subset[name], observe=args.observe)
+                gptq[name].quantizer.configure(args.wbits, perchannel=True, sym=args.sym, mse=False) # TODO: Set 'perchannel' to args.perchannel
+
+            def add_batch(name):
+
+                def tmp(_, inp, out):
+                    gptq[name].add_batch(inp[0].data, out.data)
+
+                return tmp
+
+            handles = []
+            for name in subset:
+                handles.append(subset[name].register_forward_hook(add_batch(name)))
+            for j in range(args.nsamples):
+                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]
+            for h in handles:
+                h.remove()
+
+            for name in subset:
+                scale, zero, g_idx, error = gptq[name].fasterquant(percdamp=args.percdamp, groupsize=args.groupsize, actorder=args.act_order, name=name)
+                quantizers['transformer.h.%d.%s' % (i, name)] = (gptq[name].quantizer.cpu(), scale.cpu(), zero.cpu(), g_idx.cpu(), args.wbits, args.groupsize)
+
+                if args.observe:
+                    observer.submit(name=name, layerid=i, gptq=gptq[name], error=error)
+                else:
+                    gptq[name].free()
+
+        for j in range(args.nsamples):
+            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]
+
+        layers[i] = layer.cpu()
+        del layer
+        del gptq
+
+        # TODO: Clean
+        if dev != torch.device('cpu'):
+            torch.cuda.empty_cache()
+
+        inps, outs = outs, inps
+        print('+------------------+--------------+------------+-----------+-------+')
+        print('\n')
+    model.config.use_cache = use_cache
+
+    return quantizers
+
+
+def quantize_lm_head(model, dataloader, dev, quantizers=dict()):
+
+    model = model.to(dev)
+    lm_head = model.lm_head
+
+    print('+------------------+--------------+------------+-----------+-------+')
+    print('|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |')
+    print('+==================+==============+============+===========+=======+')
+
+    gptq_lmhead = GPTQ(lm_head, observe=args.observe)
+    gptq_lmhead.quantizer.configure(args.wbits, perchannel=True, sym=args.sym, mse=False) # TODO: Set 'perchannel' to args.perchannel
+    
+    def add_batch():
+        def tmp(_, inp, out):
+            gptq_lmhead.add_batch(inp[0].data, out.data)
+        return tmp
+    
+    lm_head.register_forward_hook(add_batch())
+    for batch in dataloader:
+        transformer_outputs = model.transformer(input_ids=batch[0], attention_mask=batch[1])
+        out = lm_head(transformer_outputs[0])
+    
+    scale, zero, g_idx, error = gptq_lmhead.fasterquant(percdamp=args.percdamp, groupsize=args.groupsize, actorder=args.act_order, name="lm_head")
+    quantizers['lm_head'] = (gptq_lmhead.quantizer.cpu(), scale.cpu(), zero.cpu(), g_idx.cpu(), args.wbits, args.groupsize)
+
+    print('+------------------+--------------+------------+-----------+-------+')
+    print('\n')
+
+    return quantizers
+
+# TODO: perform packing on GPU
+def gptj_pack(model, quantizers, wbits, groupsize, quant_params_output="weight_config.json", compression_dim="K"):
+    import json
+    layers = find_layers(model)
+    
+    layers = {n: layers[n] for n in quantizers}
+    quant.make_quant_linear(model, quantizers, wbits, groupsize, compression_factor=args.compression_factor, compression_dim=compression_dim)
+    qlayers = find_layers(model, [quant.QuantLinear])
+    print('Packing ...')
+    QConfig = dict()
+    
+    for name in qlayers:
+        print(name)
+        quantizers[name], scale, zero, g_idx, _, _ = quantizers[name]
+        qlayers[name].pack(layers[name], scale, zero, g_idx)
+        QConfig[name] = qlayers[name].get_quant_params(scale, zero, g_idx)
+    print('Done.')
+
+    # No need to save params
+    # with open(quant_params_output, "w") as fid:
+    #     json.dump(QConfig, fid, indent=4)
+
+    return model
+
+if __name__ == '__main__':
+
+    parser = argparse.ArgumentParser()
+
+    parser.add_argument('--model', type=str, help='path to model checkpoint')
+
+    parser.add_argument('--nsamples', type=int, default=128, help='Number of calibration data samples.')
+    parser.add_argument('--percdamp', type=float, default=.01, help='Percent of the average Hessian diagonal to use for dampening.')
+    parser.add_argument('--nearest', action='store_true', help='Whether to run the RTN baseline.')
+    parser.add_argument('--wbits', type=int, default=16, choices=[2, 3, 4, 8, 16], help='#bits to use for quantization; use 16 for evaluating base model.')
+    parser.add_argument('--trits', action='store_true', help='Whether to use trits for quantization.')
+    parser.add_argument('--groupsize', type=int, default=-1, help='Groupsize to use for quantization; default uses full row.')
+    parser.add_argument('--save', type=str, default='', help='Save quantized checkpoint under this name.')
+
+    parser.add_argument('--sym', action='store_true', help='Whether to perform symmetric quantization.')
+    parser.add_argument('--act-order', action='store_true', help='Whether to apply the activation order GPTQ heuristic')
+    parser.add_argument('--true-sequential', action='store_true', help='Whether to run in true sequential model.')
+
+    parser.add_argument('--observe',
+                        action='store_true',
+                        help='Auto upgrade layer precision to higher precision, for example int2 to int4, groupsize 128 to 64. \
+            When this feature enabled, `--save` or `--save_safetensors` would be disable.')
+
+    parser.add_argument('--calib-data-path', type=str, help="Path to calibration json file")
+
+    parser.add_argument('--calib-iters', type=int, default=100, help="Number of samples for calibration")
+    parser.add_argument("--quant-config-output", type=str, default="quant-params.json", help="Where to save quantization scales and zeros")
+    parser.add_argument("--quantize-lm-head", action='store_true', help="Whether to quantize the lm_head (the output layer) in addition to transformer layers")
+    parser.add_argument("--compression-factor", type=int, default=8, help="Compressoin factor for quantized weights")
+    parser.add_argument("--compression-dim", choices=["K", "N"], default="K", help="Dimension along which to compress. Assumption: Layer weight shape: [out_features, in_features] ---> [N, K]")
+
+    args = parser.parse_args()
+
+    model = get_gptj(args.model)
+    model.eval()
+    calib_dataset = CNNDAILYMAIL(args.model, args.calib_data_path,is_calib=True,num_samples=args.calib_iters)
+    dataloader=DataLoader(calib_dataset,
+        batch_size=1,
+        shuffle=False,
+        collate_fn=calib_dataset.collate_batch
+    )
+    try:
+        quantizers = {}
+        quantizers = gptj_sequential(model, dataloader, DEV, quantizers)
+        if args.quantize_lm_head:
+            quantizers = quantize_lm_head(model, dataloader, DEV, quantizers )
+        
+        gptj_pack(model, quantizers, args.wbits, args.groupsize, quant_params_output=args.quant_config_output, compression_dim=args.compression_dim)
+        os.makedirs(os.path.dirname(args.save), exist_ok=True)
+        torch.save(model.state_dict(), args.save)
+        print("Model saved at {}".format(args.save))
+    except Exception as e:
+        print("Quantization failed: {}".format(e))
diff --git a/gptq.py b/gptq.py
index b1be26c..b1c7090 100644
--- a/gptq.py
+++ b/gptq.py
@@ -138,7 +138,7 @@ class GPTQ:
         tick = time.time()
 
         if not self.quantizer.ready():
-            self.quantizer.find_params(W, weight=True)
+            self.quantizer.find_params(W, weight=True) # Computes scale and zero parameters for the entier weight
 
         H = self.H
         if not self.observe:
@@ -192,7 +192,7 @@ class GPTQ:
                         now_idx += 1
 
                 q = self.quantizer.quantize(w.unsqueeze(1)).flatten()
-                Q1[:, i] = q
+                Q1[:, i] = q # The approximation to w
                 Losses1[:, i] = (w - q)**2 / d**2
 
                 err1 = (w - q) / d
@@ -204,7 +204,9 @@ class GPTQ:
 
             W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])
 
-        torch.cuda.synchronize()
+        if self.dev != torch.device('cpu'):
+            torch.cuda.synchronize()
+
         error = torch.sum(Losses).item()
 
         groupsize = groupsize if groupsize != -1 else self.columns
@@ -233,4 +235,5 @@ class GPTQ:
         self.H = None
         self.Losses = None
         self.Trace = None
-        torch.cuda.empty_cache()
+        if self.dev != torch.device('cpu'):
+            torch.cuda.empty_cache()
diff --git a/quant/__init__.py b/quant/__init__.py
index 6445278..5578f3a 100644
--- a/quant/__init__.py
+++ b/quant/__init__.py
@@ -1,5 +1,3 @@
 from .quantizer import Quantizer
-from .fused_attn import QuantLlamaAttention, make_quant_attn
-from .fused_mlp import QuantLlamaMLP, make_fused_mlp, autotune_warmup_fused
 from .quant_linear import QuantLinear, make_quant_linear, autotune_warmup_linear
-from .triton_norm import TritonLlamaRMSNorm, make_quant_norm
+
diff --git a/quant/quant_linear.py b/quant/quant_linear.py
index 0c77021..c92431a 100644
--- a/quant/quant_linear.py
+++ b/quant/quant_linear.py
@@ -2,7 +2,7 @@ import math
 import numpy as np
 import torch
 import torch.nn as nn
-from torch.cuda.amp import custom_bwd, custom_fwd
+#from torch.cuda.amp import custom_bwd, custom_fwd
 
 try:
     import triton
@@ -261,28 +261,28 @@ except:
 
 
 def matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq):
-    with torch.cuda.device(input.device):
-        output = torch.empty((input.shape[0], qweight.shape[1]), device=input.device, dtype=torch.float16)
-        grid = lambda META: (triton.cdiv(input.shape[0], META['BLOCK_SIZE_M']) * triton.cdiv(qweight.shape[1], META['BLOCK_SIZE_N']), )
-        matmul_248_kernel[grid](input, qweight, output, scales, qzeros, g_idx, input.shape[0], qweight.shape[1], input.shape[1], bits, maxq, input.stride(0), input.stride(1), qweight.stride(0),
-                                qweight.stride(1), output.stride(0), output.stride(1), scales.stride(0), qzeros.stride(0))
-        return output
+    #with torch.cuda.device(input.device):
+    output = torch.empty((input.shape[0], qweight.shape[1]), device=input.device, dtype=torch.float16)
+    grid = lambda META: (triton.cdiv(input.shape[0], META['BLOCK_SIZE_M']) * triton.cdiv(qweight.shape[1], META['BLOCK_SIZE_N']), )
+    matmul_248_kernel[grid](input, qweight, output, scales, qzeros, g_idx, input.shape[0], qweight.shape[1], input.shape[1], bits, maxq, input.stride(0), input.stride(1), qweight.stride(0),
+                            qweight.stride(1), output.stride(0), output.stride(1), scales.stride(0), qzeros.stride(0))
+    return output
 
 
 def transpose_matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq):
-    with torch.cuda.device(input.device):
-        output_dim = (qweight.shape[0] * 32) // bits
-        output = torch.empty((input.shape[0], output_dim), device=input.device, dtype=torch.float16)
-        grid = lambda META: (triton.cdiv(input.shape[0], META['BLOCK_SIZE_M']) * triton.cdiv(output_dim, META['BLOCK_SIZE_K']), )
-        transpose_matmul_248_kernel[grid](input, qweight, output, scales, qzeros, g_idx, input.shape[0], qweight.shape[1], output_dim, bits, maxq, input.stride(0), input.stride(1), qweight.stride(0),
-                                          qweight.stride(1), output.stride(0), output.stride(1), scales.stride(0), qzeros.stride(0))
-        return output
+    #with torch.cuda.device(input.device):
+    output_dim = (qweight.shape[0] * 32) // bits
+    output = torch.empty((input.shape[0], output_dim), device=input.device, dtype=torch.float16)
+    grid = lambda META: (triton.cdiv(input.shape[0], META['BLOCK_SIZE_M']) * triton.cdiv(output_dim, META['BLOCK_SIZE_K']), )
+    transpose_matmul_248_kernel[grid](input, qweight, output, scales, qzeros, g_idx, input.shape[0], qweight.shape[1], output_dim, bits, maxq, input.stride(0), input.stride(1), qweight.stride(0),
+                                      qweight.stride(1), output.stride(0), output.stride(1), scales.stride(0), qzeros.stride(0))
+    return output
 
 
 class QuantLinearFunction(torch.autograd.Function):
 
     @staticmethod
-    @custom_fwd(cast_inputs=torch.float16)
+    #@custom_fwd(cast_inputs=torch.float16)
     def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq):
         output = matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq)
         ctx.save_for_backward(qweight, scales, qzeros, g_idx)
@@ -290,7 +290,7 @@ class QuantLinearFunction(torch.autograd.Function):
         return output
 
     @staticmethod
-    @custom_bwd
+    #@custom_bwd
     def backward(ctx, grad_output):
         qweight, scales, qzeros, g_idx = ctx.saved_tensors
         bits, maxq = ctx.bits, ctx.maxq
@@ -303,7 +303,7 @@ class QuantLinearFunction(torch.autograd.Function):
 
 class QuantLinear(nn.Module):
 
-    def __init__(self, bits, groupsize, infeatures, outfeatures, bias):
+    def __init__(self, bits, groupsize, infeatures, outfeatures, bias, compression_factor=2, compression_dim="K"):
         super().__init__()
         if bits not in [2, 4, 8]:
             raise NotImplementedError("Only 2,4,8 bits are supported.")
@@ -312,18 +312,39 @@ class QuantLinear(nn.Module):
         self.bits = bits
         self.maxq = 2**self.bits - 1
         self.groupsize = groupsize if groupsize != -1 else infeatures
+        self.compression_factor = compression_factor
+        self.compression_dtype = {2: (torch.uint8, np.uint8), 8: (torch.int32, np.int32)}
+        self.param_dtype = self.compression_dtype[self.compression_factor]
+        self.compression_dim = compression_dim
+
+        if self.compression_dim=="K":
+            q,r = divmod(infeatures, self.compression_factor)
+            self.compressed_shape = (q+r, outfeatures)
+        else:
+            q,r = divmod(outfeatures, self.compression_factor)
+            self.compressed_shape = (infeatures, q+r)
+
+        self.register_buffer('qweight', torch.zeros(self.compressed_shape, dtype=self.param_dtype[0]))
+        
+        q,r = divmod(outfeatures, self.compression_factor)
+        self.register_buffer('qzeros', torch.zeros((math.ceil(infeatures / self.groupsize), q+r), dtype=self.param_dtype[0]))
 
-        self.register_buffer('qweight', torch.zeros((infeatures // 32 * self.bits, outfeatures), dtype=torch.int32))
-        self.register_buffer('qzeros', torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures // 32 * self.bits), dtype=torch.int32))
         self.register_buffer('scales', torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures), dtype=torch.float16))
-        self.register_buffer('g_idx', torch.tensor([i // self.groupsize for i in range(infeatures)], dtype=torch.int32))
+        self.register_buffer('group_size', torch.tensor(self.groupsize, dtype=torch.int)) # Add 'group_size' and remove 'g_idx'
         if bias:
             self.register_buffer('bias', torch.zeros((outfeatures), dtype=torch.float16))
         else:
             self.bias = None
 
+        self.config = dict()
+
+    def get_quant_params(self, scales, zeros, g_idx):
+        self.config["scale"] = dict((i, scales[i].tolist()) for i in range(scales.shape[0]))
+        self.config["zero_point"] = dict((i, zeros[i].tolist()) for i in range(zeros.shape[0]))
+
+        return self.config
+
     def pack(self, linear, scales, zeros, g_idx=None):
-        self.g_idx = g_idx.clone() if g_idx is not None else self.g_idx
 
         scales = scales.t().contiguous()
         zeros = zeros.t().contiguous()
@@ -334,50 +355,75 @@ class QuantLinear(nn.Module):
 
         intweight = []
         for idx in range(self.infeatures):
-            intweight.append(torch.round((linear.weight.data[:, idx] + scale_zeros[self.g_idx[idx]]) / self.scales[self.g_idx[idx]]).to(torch.int)[:, None])
+            intweight.append(torch.round((linear.weight.data[:, idx] + scale_zeros[g_idx[idx]]) / self.scales[g_idx[idx]]).to(torch.int)[:, None])
         intweight = torch.cat(intweight, dim=1)
         intweight = intweight.t().contiguous()
         intweight = intweight.numpy().astype(np.uint32)
-        qweight = np.zeros((intweight.shape[0] // 32 * self.bits, intweight.shape[1]), dtype=np.uint32)
+
+        
+        qweight = np.zeros(self.compressed_shape, dtype=self.param_dtype[1]) #int32)
+        if self.compression_dim=="N":
+            intweight = intweight.transpose()
+            qweight = qweight.transpose()
+
+        # print("Qweight temp shape: {}".format(qweight.shape))
+        # print("intweight shape: {}".format(intweight.shape))
         i = 0
         row = 0
         while row < qweight.shape[0]:
             if self.bits in [2, 4, 8]:
-                for j in range(i, i + (32 // self.bits)):
+                j=i
+                while j < min(i + self.compression_factor, intweight.shape[0]):
+                
                     qweight[row] |= intweight[j] << (self.bits * (j - i))
-                i += 32 // self.bits
+                    j += 1
+                i += self.compression_factor
                 row += 1
             else:
                 raise NotImplementedError("Only 2,4,8 bits are supported.")
 
-        qweight = qweight.astype(np.int32)
-        self.qweight = torch.from_numpy(qweight)
+        qweight = qweight.astype(self.param_dtype[1]) #np.int32)
+        
+        if self.compression_dim=="N":
+            qweight = qweight.transpose()
 
+        print("Compressed along {}. Compressed from {} to {}".format(self.compression_dim, list(linear.weight.data.shape), list(self.compressed_shape)))
+        
+        self.qweight = torch.from_numpy(qweight)
+        
         zeros -= 1
         zeros = zeros.numpy().astype(np.uint32)
-        qzeros = np.zeros((zeros.shape[0], zeros.shape[1] // 32 * self.bits), dtype=np.uint32)
+        q,r = divmod(zeros.shape[1], self.compression_factor)
+        
+        qzeros = np.zeros((zeros.shape[0], q+r ), dtype=self.param_dtype[1]) #np.uint32)
         i = 0
         col = 0
         while col < qzeros.shape[1]:
             if self.bits in [2, 4, 8]:
-                for j in range(i, i + (32 // self.bits)):
+                j=i
+                # for j in range(i, i + self.compression_factor):
+                while j < min(i + self.compression_factor, zeros.shape[1]):
                     qzeros[:, col] |= zeros[:, j] << (self.bits * (j - i))
-                i += 32 // self.bits
+                    j += 1
+                i += self.compression_factor
                 col += 1
             else:
                 raise NotImplementedError("Only 2,4,8 bits are supported.")
 
-        qzeros = qzeros.astype(np.int32)
+        qzeros = qzeros.astype(self.param_dtype[1]) #np.int32)
         self.qzeros = torch.from_numpy(qzeros)
 
     def forward(self, x):
-        out_shape = x.shape[:-1] + (self.outfeatures, )
-        out = QuantLinearFunction.apply(x.reshape(-1, x.shape[-1]), self.qweight, self.scales, self.qzeros, self.g_idx, self.bits, self.maxq)
-        out = out + self.bias if self.bias is not None else out
-        return out.reshape(out_shape)
+        return x # Implement this
+    
+        # out_shape = x.shape[:-1] + (self.outfeatures, )
+        # out = QuantLinearFunction.apply(x.reshape(-1, x.shape[-1]), self.qweight, self.scales, self.qzeros, self.g_idx, self.bits, self.maxq)
+        # out = out + self.bias if self.bias is not None else out
+        # return out.reshape(out_shape)
 
 
-def make_quant_linear(module, names, bits, groupsize, name=''):
+def make_quant_linear(module, names, bits, groupsize, name='', compression_factor=2, compression_dim="K"):
+    # print("Compression dim: {}".format(compression_dim))
     if isinstance(module, QuantLinear):
         return
     for attr in dir(module):
@@ -385,9 +431,9 @@ def make_quant_linear(module, names, bits, groupsize, name=''):
         name1 = name + '.' + attr if name != '' else attr
         if name1 in names:
             delattr(module, attr)
-            setattr(module, attr, QuantLinear(bits, groupsize, tmp.in_features, tmp.out_features, tmp.bias is not None))
+            setattr(module, attr, QuantLinear(bits, groupsize, tmp.in_features, tmp.out_features, tmp.bias is not None, compression_factor=compression_factor, compression_dim=compression_dim))
     for name1, child in module.named_children():
-        make_quant_linear(child, names, bits, groupsize, name + '.' + name1 if name != '' else name1)
+        make_quant_linear(child, names, bits, groupsize, name + '.' + name1 if name != '' else name1, compression_factor=compression_factor, compression_dim=compression_dim)
 
 
 def autotune_warmup_linear(model, transpose=False):
@@ -406,7 +452,8 @@ def autotune_warmup_linear(model, transpose=False):
         n = m.outfeatures
 
         if (k, n) not in kn_values:
-            kn_values[(k, n)] = (m.qweight.cuda(), m.scales.cuda(), m.qzeros.cuda(), m.g_idx.cuda(), m.bits, m.maxq)
+            #kn_values[(k, n)] = (m.qweight.cuda(), m.scales.cuda(), m.qzeros.cuda(), m.g_idx.cuda(), m.bits, m.maxq)
+            kn_values[(k, n)] = (m.qweight, m.scales, m.qzeros, m.g_idx, m.bits, m.maxq)
 
     print(f'Found {len(kn_values)} unique KN Linear values.')
 
@@ -415,9 +462,9 @@ def autotune_warmup_linear(model, transpose=False):
         for m in tqdm(range(0, 12)):
             m = 2**m  # [1, 2048]
             for (k, n), (qweight, scales, qzeros, g_idx, bits, maxq) in kn_values.items():
-                a = torch.randn(m, k, dtype=torch.float16, device='cuda')
+                a = torch.randn(m, k, dtype=torch.bfloat16, device='cpu')
                 matmul248(a, qweight, scales, qzeros, g_idx, bits, maxq)
                 if transpose:
-                    a = torch.randn(m, n, dtype=torch.float16, device='cuda')
+                    a = torch.randn(m, n, dtype=torch.bfloat16, device='cpu')
                     transpose_matmul248(a, qweight, scales, qzeros, g_idx, bits, maxq)
     del kn_values
diff --git a/utils/__init__.py b/utils/__init__.py
index cf17412..200444d 100644
--- a/utils/__init__.py
+++ b/utils/__init__.py
@@ -1,3 +1 @@
 from .modelutils import DEV, find_layers, gen_conditions, torch_snr_error
-from .datautils import set_seed, get_wikitext2, get_ptb, get_c4, get_ptb_new, get_c4_new, get_loaders
-from .export import export_quant_table
diff --git a/utils/modelutils.py b/utils/modelutils.py
index d043cca..10f4106 100644
--- a/utils/modelutils.py
+++ b/utils/modelutils.py
@@ -1,7 +1,8 @@
 import torch
 import torch.nn as nn
 
-DEV = torch.device('cuda:0')
+#DEV = torch.device('cuda:0')
+DEV = torch.device('cpu')
 
 
 def find_layers(module, layers=[nn.Conv2d, nn.Linear], name=''):
