{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc20f568",
   "metadata": {},
   "source": [
    "## Example: Migrate files from AWS S3 bucket to GCP Storage and Azure Blob Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39d0f9",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "1. Install Cloud Data Connector.\n",
    "\n",
    "For this example, create a bucket in AWS S3, GCP Storage and Azure Blob Storage.\n",
    "\n",
    "Before running the notebook, configure environment variables and set your credentials as follows:\n",
    "\n",
    "#### AWS\n",
    "```\n",
    "$ export AWS_ACCESS_KEY_ID=<your_key_id>\n",
    "$ export AWS_SECRET_ACCESS_KEY=<your_secret_key>\n",
    "$ export AWS_BUCKET_NAME=<your_bucket_name>\n",
    "```\n",
    "\n",
    "#### GCP\n",
    "```\n",
    "$ export GOOGLE_APPLICATION_CREDENTIALS=path/to/your_key_id.json\n",
    "$ export GCP_PROJECT_NAME=<your_gcp_project_name>\n",
    "$ export GCP_BUCKET_NAME=<your_gcp_project_name>\n",
    "```\n",
    "\n",
    "#### Azure\n",
    "```\n",
    "$ export AZURE_BLOB_NAME=<your_azure_blob_name>\n",
    "$ export AZURE_CONNECTION_STRING=<your_azure_connection_string>\n",
    "```\n",
    "\n",
    "### Specify key value for the new file you will upload to AWS S3 bucket\n",
    "\n",
    "In AWS S3, files are stored in buckets. S3 supports the folder concept as a means of grouping objects, so you can specify a folder name where to put a file as a key. For example, a key should be `dir_name/file_name`.\n",
    "\n",
    "Below cell defines a key for this example. The file will be saved in `1937` folder and its name will be `hello_world.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16438501",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"1937\"\n",
    "file_name = \"hello_world.txt\"\n",
    "key = f\"{dir_name}/{file_name}\"\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d2f41",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "Create a downloads directory to save downloaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4725ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "download_dir = 'downloads'\n",
    "if not os.path.exists(download_dir):\n",
    "    os.mkdir(download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258e1f8",
   "metadata": {},
   "source": [
    "Create a uploads directory to save all files you will upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291edce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploads_dir = 'uploads'\n",
    "if not os.path.exists(uploads_dir):\n",
    "    os.mkdir(uploads_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a0fb3",
   "metadata": {},
   "source": [
    "Create a txt file in uploads directory and add a plain text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text = \"Hello World!\"\n",
    "file_path = f\"{uploads_dir}/{file_name}\"\n",
    "with open(file_path, \"w\", encoding=\"UTF-8\") as f:\n",
    "    f.write(file_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04692ff7",
   "metadata": {},
   "source": [
    "### Migrate data with Cloud Data Connector\n",
    "\n",
    "Read bucket name from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a7ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aws_bucket_name = os.environ[\"BUCKET_NAME\"]\n",
    "except KeyError: \n",
    "    print(\"Environment variable does not exist, please set a value for aws_bucket_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a66bcee",
   "metadata": {},
   "source": [
    "#### Upload file to AWS S3\n",
    "\n",
    "Import `Connector` and `Uploader` from data_connector package. Create a `Connector` to get a S3 client. By default, the `connect` function reads the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` values from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34ad533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloud_data_connector.aws import Connector, Uploader\n",
    "s3_client = Connector().connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c34afb",
   "metadata": {},
   "source": [
    "Next step is to create an `Uploader`, add the S3 client returned by `connect` as parameter and call to `upload`. Set bucket name, file name and key parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b73468",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uploader = Uploader(s3_client)\n",
    "s3_uploader.upload(aws_bucket_name, file_path, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042fbaa",
   "metadata": {},
   "source": [
    "### Download file with Cloud Data Connector from AWS S3 bucket\n",
    "\n",
    "Download file `hello_world.txt` and save it in `downloads/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8bd1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloud_data_connector.aws import Downloader\n",
    "s3_downloader = Downloader(s3_client)\n",
    "s3_downloader.download(aws_bucket_name, key, f\"{download_dir}/{file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba31a8c",
   "metadata": {},
   "source": [
    "### Upload file to GCP bucket\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346b74a6",
   "metadata": {},
   "source": [
    "Read credentials from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4360275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    gcp_app_credentials = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]\n",
    "    gcp_project_name = os.environ[\"GCP_PROJECT_NAME\"]\n",
    "    gcp_bucket_name = os.environ[\"GCP_BUCKET_NAME\"]\n",
    "except KeyError as error:\n",
    "    print(f\"Environment variable does not exist, please set a value for {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44996d3",
   "metadata": {},
   "source": [
    "You can reuse all code from above cells to create a GCP Connector and Uploader. Change the cloud provider in export instruction to use `gcp` and add required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a7a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloud_data_connector.gcp import Connector, Uploader\n",
    "gcp_storage_client = Connector(\"storage\").connect(connection_string=gcp_project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabff29f",
   "metadata": {},
   "source": [
    "Create an Upload is identical like AWS, the only diference is the GCP client as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a5e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp_uploader = Uploader(gcp_storage_client)\n",
    "gcp_uploader.upload_to_bucket(gcp_bucket_name, f\"{download_dir}/{file_name}\", f\"{dir_name}/{file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a25bbc0",
   "metadata": {},
   "source": [
    "### Upload file to Azure Blob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45f27be",
   "metadata": {},
   "source": [
    "Read credentials from environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    azure_blob_name = os.environ[\"AZURE_BLOB_NAME\"]\n",
    "    azure_connection_string = os.environ[\"AZURE_CONNECTION_STRING\"]\n",
    "except KeyError as error:\n",
    "    print(f\"Environment variable does not exist, please set a value for {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850fd11d",
   "metadata": {},
   "source": [
    "Prepare file to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text = \"Hello World!\"\n",
    "uploads_dir=\"1937\"\n",
    "file_path = f\"{uploads_dir}/{file_name}\"\n",
    "if not os.path.exists(uploads_dir):\n",
    "    os.mkdir(uploads_dir)\n",
    "with open(file_path, \"w\", encoding=\"UTF-8\") as f:\n",
    "    f.write(file_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0050678e",
   "metadata": {},
   "source": [
    "Reuse the code for GCP, change the export instruction to Azure and add all required parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d14a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloud_data_connector.azure import Connector, Uploader\n",
    "azure_storage_client = Connector().connect(connection_string=azure_connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc44264",
   "metadata": {},
   "source": [
    "Create an Uploader and add the storage client created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1873aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_uploader = Uploader(azure_storage_client)\n",
    "azure_uploader.upload(f\"{uploads_dir}/{file_name}\", azure_blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ebee4f",
   "metadata": {},
   "source": [
    "### Migrate data without Cloud Data Connector\n",
    "\n",
    "#### Upload file to AWS S3 bucket\n",
    "\n",
    "Upload file from uploads_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "uploads_dir=\"uploads\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7ac9b5",
   "metadata": {},
   "source": [
    "Create a S3 client with boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb2dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832abc58",
   "metadata": {},
   "source": [
    "Add all required parameters to upload_file function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.upload_file(f\"{uploads_dir}/{file_name}\", aws_bucket_name, f\"{dir_name}/{file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d919ff8c",
   "metadata": {},
   "source": [
    "Download the file to download_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984c352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(aws_bucket_name, f\"{dir_name}/{file_name}\", f\"{download_dir}/{file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05509581",
   "metadata": {},
   "source": [
    "#### Upload file to GCP\n",
    "\n",
    "Import storage package and create a GCP Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a805e",
   "metadata": {},
   "source": [
    "To upload a file, you need to create a bucket and a blob object. Execute the upload_from_filename method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage_client.bucket(gcp_bucket_name)\n",
    "blob = bucket.blob(f\"{dir_name}/{file_name}\")\n",
    "blob.upload_from_filename(f\"{download_dir}/{file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa4d0d",
   "metadata": {},
   "source": [
    "### Upload file to Azure\n",
    "\n",
    "Create a BlobServiceClient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a64f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(azure_connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1de1807",
   "metadata": {},
   "source": [
    "Create a container client and upload the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ebed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_client = blob_service_client.get_container_client(container=azure_blob_name)\n",
    "with open(file=os.path.join(download_dir, file_name), mode=\"rb\") as data:\n",
    "    blob_client = container_client.upload_blob(name=f\"{dir_name}/{file_name}\", data=data, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68930783",
   "metadata": {},
   "source": [
    "### Notes\n",
    "With Cloud Data Connector, there is a common import instruction for AWS, GCP and Azure, just specify the cloud provider name and set required parameters to create a Connector. However, without it you need to import `boto3`, `google.cloud` and `azure.storage.blob`.\n",
    "\n",
    "With Cloud Data Connector, there is a common connect method to get a client for AWS S3, GCP Storage and Azure Blob, just add a connection string, project name or leave connect function reads your credentials from your environment variables.\n",
    "\n",
    "With Cloud Data Connector, there is a common upload method, just need to set the cloud client to create a `Uploader` and call `upload`.\n",
    "\n",
    "The next examples show lines of codes needed to upload a file with Cloud Data Connector and with AWS, GCP and Azure SDK for Python. \n",
    "\n",
    "Code to upload a file with Cloud Data Connector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b7758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloud_data_connector import aws, gcp, azure\n",
    "s3_client = aws.Connector().connect(connection_string=\"\")\n",
    "gcp_client = gcp.Connector(\"storage\").connect(connection_string=gcp_project_name)\n",
    "azure_client = azure.Connector().connect(connection_string=azure_connection_string)\n",
    "aws.Uploader(s3_client).upload(aws_bucket_name, file_path, key)\n",
    "gcp.Uploader(gcp_client).upload_to_bucket(gcp_bucket_name, f\"{download_dir}/{file_name}\", f\"{dir_name}/{file_name}\")\n",
    "azure.Uploader(azure_client).upload(f\"{uploads_dir}/{file_name}\", azure_blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e29430",
   "metadata": {},
   "source": [
    "Code to upload a file to with boto3, google-cloud and azure blob storage clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from google.cloud import storage\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "s3_client = boto3.client('s3')\n",
    "gcp_storage_client = storage.Client()\n",
    "azure_blob_client = BlobServiceClient.from_connection_string(azure_connection_string)\n",
    "s3_client.upload_file(f\"{uploads_dir}/{file_name}\", aws_bucket_name, f\"{dir_name}/{file_name}\")\n",
    "bucket = gcp_storage_client.bucket(gcp_bucket_name)\n",
    "blob = bucket.blob(f\"{dir_name}/{file_name}\")\n",
    "blob.upload_from_filename(f\"{download_dir}/{file_name}\")\n",
    "container_client = azure_blob_client.get_container_client(container=azure_blob_name)\n",
    "with open(file=os.path.join(download_dir, file_name), mode=\"rb\") as data:\n",
    "    blob_client = container_client.upload_blob(name=f\"{dir_name}/{file_name}\", data=data, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intel_sample_env",
   "language": "python",
   "name": "intel_sample_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
