releases:
  versioned:
    tag_specs:
    - '{pytorch-ipex-gpu-base-public}{pytorch-atsm-ssd-mobilenet-inference}'
slice_sets:
  pytorch-atsm-ssd-mobilenet-inference:
  - add_to_name: pytorch-atsm-ssd-mobilenet-inference
    dockerfile_exclusive_name: -ssd-mobilenet-inference
    args:
    - PACKAGE_NAME=pytorch-atsm-ssd-mobilenet-inference
    dockerfile_subdirectory: gpu_model_containers
    documentation:
      - docs:
        - name: Title
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/title.md
        - name: Description
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/description.md
        - name: GPU Setup
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/gpu_setup.md
        - name: Datasets
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/datasets.md
        - name: Quick Start Scripts
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/quickstart.md
        - name: Baremetal
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/baremetal.md
        - name: License
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/license.md
        name: README.md
        text_replace:
          <device>: Intel® Data Center GPU Flex Series
          <mode>: inference
          <model name>: SSD-Mobilenetv1
          <package dir>: pytorch-atsm-ssd-mobilenet-inference
          <use case>: object_detection
        uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu
      - docs:
        - name: Title
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/title.md
        - name: Description
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/description.md
        - name: Package
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/package.md
        - name: Datasets
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/datasets.md
        - name: Quick Start Scripts
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/quickstart.md
        - name: Docker
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/docker.md
        - name: License
          uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/.docs/license.md
        name: wrapper_README.md
        text_replace:
          <device>: Intel® Data Center GPU Flex Series
          <mode>: inference
          <model name>: SSD-Mobilenetv1
          <package dir>: pytorch-atsm-ssd-mobilenet-inference
          <package name>: pytorch-atsm-ssd-mobilenet-inference.tar.gz
          <use case>: object_detection
        uri: models/quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu
    downloads:
    - destination: labels/voc-model-labels.txt
      source: https://storage.googleapis.com/models-hao/voc-model-labels.txt
    files:
    - source: quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/README.md
      destination: README.md
    - destination: models/object_detection/pytorch/ssd-mobilenet/inference/gpu
      source: models/object_detection/pytorch/ssd-mobilenet/inference/gpu
    - destination: quickstart/setup.sh
      source: quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/setup.sh
    - destination: quickstart/inference_with_dummy_data.sh
      source: quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/inference_with_dummy_data.sh
    - destination: quickstart/setup.sh
      source: quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/setup.sh
    wrapper_package_files:
     - source: quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/README.md
       destination: README.md
     - source: quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/build.sh
       destination: build.sh
     - source: quickstart/object_detection/pytorch/ssd-mobilenet/inference/gpu/run.sh
       destination: run.sh
     - source: dockerfiles/gpu_model_containers/pytorch-atsm-ssd-mobilenet-inference.Dockerfile
       destination: pytorch-atsm-ssd-mobilenet-inference.Dockerfile
     - source: output/pytorch-atsm-ssd-mobilenet-inference.tar.gz
       destination: model_packages/pytorch-atsm-ssd-mobilenet-inference.tar.gz
     - source: ""
       destination: info.txt
     - source: third_party
       destination: licenses/third_party
     - source: LICENSE
       destination: licenses/LICENSE
    partials:
    - numactl
    - python-build
    - opencv
    - object_detection/pip_installs
    - model_package
    - entrypoint
