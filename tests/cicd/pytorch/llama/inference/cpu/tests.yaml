inference-fp32-throughput:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "fp32"
    TEST_MODE: "THROUGHPUT"
    BATCH_SIZE: "256"
    CORE_PER_INSTANCE: ""
inference-bf32-throughput:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "bf32"
    TEST_MODE: "THROUGHPUT"
    BATCH_SIZE: "256"
    CORE_PER_INSTANCE: ""
inference-bf16-throughput:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "bf16"
    TEST_MODE: "THROUGHPUT"
    BATCH_SIZE: "256"
    CORE_PER_INSTANCE: ""
inference-fp16-throughput:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "fp16"
    TEST_MODE: "THROUGHPUT"
    BATCH_SIZE: "256"
    CORE_PER_INSTANCE: ""
inference-int8-fp32-throughput:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "int8-fp32"
    TEST_MODE: "THROUGHPUT"
    BATCH_SIZE: "256"
    CORE_PER_INSTANCE: ""
inference-int8-bf16-throughput:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "int8-bf16"
    TEST_MODE: "THROUGHPUT"
    BATCH_SIZE: "256"
    CORE_PER_INSTANCE: ""
inference-fp32-accuracy:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "fp32"
    TEST_MODE: "ACCURACY"
    BATCH_SIZE: "256"
    CORE_PER_INSTANCE: ""
inference-bf32-accuracy:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "bf32"
    TEST_MODE: "ACCURACY"
    BATCH_SIZE: "256"
    CORE_PER_INSTANCE: ""
inference-bf16-accuracy:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "bf16"
    TEST_MODE: "ACCURACY"
    BATCH_SIZE: "256"
    CORE_PER_INSTANCE: ""
inference-fp16-accuracy:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "fp16"
    TEST_MODE: "ACCURACY"
    BATCH_SIZE: "256"
    CORE_PER_INSTANCE: ""
inference-int8-fp32-accuracy:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "int8-fp32"
    TEST_MODE: "ACCURACY"
    BATCH_SIZE: "256"
    CORE_PER_INSTANCE: ""
inference-int8-bf16-accuracy:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "int8-bf16"
    TEST_MODE: "ACCURACY"
    BATCH_SIZE: "256"
    CORE_PER_INSTANCE: ""
inference-fp32-realtime:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "fp32"
    TEST_MODE: "REALTIME"
    BATCH_SIZE: "1"
    CORE_PER_INSTANCE: "4"
inference-bf32-realtime:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "bf32"
    TEST_MODE: "REALTIME"
    BATCH_SIZE: "1"
    CORE_PER_INSTANCE: "4"
inference-bf16-realtime:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "bf16"
    TEST_MODE: "REALTIME"
    BATCH_SIZE: "1"
    CORE_PER_INSTANCE: "4"
inference-fp16-realtime:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "fp16"
    TEST_MODE: "REALTIME"
    BATCH_SIZE: "1"
    CORE_PER_INSTANCE: "4"
inference-int8-fp32-realtime:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "int8-fp32"
    TEST_MODE: "REALTIME"
    BATCH_SIZE: "1"
    CORE_PER_INSTANCE: "4"
inference-int8-bf16-realtime:
  cmd:
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/pyt_cpu_setup.sh ${FRAMEWORK_VERSION} ${IS_LKG_DROP} ${AIKIT_VERSION};
    bash $GITHUB_WORKSPACE/tests/cicd/pytorch/llama/inference/cpu/test_model.sh ${PRECISION} ${IS_LKG_DROP} ${TEST_MODE} ${BATCH_SIZE} ${CORE_PER_INSTANCE}
  env:
    PRECISION: "int8-bf16"
    TEST_MODE: "REALTIME"
    BATCH_SIZE: "1"
    CORE_PER_INSTANCE: "4"
