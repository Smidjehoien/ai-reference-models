fp32-training-phase1:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-training
  cmd: quickstart/run_bert_pretrain_phase1.sh fp32
  shm-size: 8G
  env:
    PRECISION: fp32
    BERT_MODEL_CONFIG: /pytorch/enwiki-20200101/bert_config.json
    OUTPUT_DIR: /tmp
    TRAIN_SCRIPT: /workspace/pytorch-bert-large-training/models/language_modeling/pytorch/bert_large/training/run_pretrain_mlperf.py
    EVAL_SCRIPT: /workspace/pytorch-bert-large-training/quickstart/transformers/examples/legacy/question-answering/run_squad.py
    DATASET_DIR: /pytorch/tfrecord_dir/bert_results4_dataset/
  volumes:
    - src: /pytorch/enwiki-20200101/bert_config.json
      dst: /pytorch/enwiki-20200101/bert_config.json
    - src: /tmp
      dst: /tmp
    - src: /pytorch/tfrecord_dir/bert_results4_dataset/
      dst: /pytorch/tfrecord_dir/bert_results4_dataset/
fp32-training-phase2:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-training
  cmd: quickstart/run_bert_pretrain_phase2.sh fp32
  shm-size: 8G
  env:
    PRECISION: fp32
    BERT_MODEL_CONFIG: /pytorch/enwiki-20200101/bert_config.json
    OUTPUT_DIR: /tmp
    PRETRAINED_MODEL: /tmp/model_save
    TRAIN_SCRIPT: /workspace/pytorch-bert-large-training/models/language_modeling/pytorch/bert_large/training/run_pretrain_mlperf.py
    EVAL_SCRIPT: /workspace/pytorch-bert-large-training/quickstart/transformers/examples/legacy/question-answering/run_squad.py
    DATASET_DIR: /pytorch/tfrecord_dir/bert_results4_dataset/
  volumes:
    - src: /pytorch/enwiki-20200101/bert_config.json
      dst: /pytorch/enwiki-20200101/bert_config.json
    - src: /tmp/model_save
      dst: /tmp/model_save
    - src: /pytorch/tfrecord_dir/bert_results4_dataset/
      dst: /pytorch/tfrecord_dir/bert_results4_dataset/
bf16-training-phase1:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-training
  cmd: quickstart/run_bert_pretrain_phase1.sh bf16
  shm-size: 8G
  env:
    PRECISION: bf16
    BERT_MODEL_CONFIG: /pytorch/enwiki-20200101/bert_config.json
    OUTPUT_DIR: /tmp
    TRAIN_SCRIPT: /workspace/pytorch-bert-large-training/models/language_modeling/pytorch/bert_large/training/run_pretrain_mlperf.py
    EVAL_SCRIPT: /workspace/pytorch-bert-large-training/quickstart/transformers/examples/legacy/question-answering/run_squad.py
    DATASET_DIR: /pytorch/tfrecord_dir/bert_results4_dataset/
  volumes:
    - src: /pytorch/enwiki-20200101/bert_config.json
      dst: /pytorch/enwiki-20200101/bert_config.json
    - src: /tmp
      dst: /tmp
    - src: /pytorch/tfrecord_dir/bert_results4_dataset/
      dst: /pytorch/tfrecord_dir/bert_results4_dataset/
bf16-training-phase2:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-training
  cmd: quickstart/run_bert_pretrain_phase2.sh bf16
  shm-size: 8G
  env:
    PRECISION: bf16
    BERT_MODEL_CONFIG: /pytorch/enwiki-20200101/bert_config.json
    OUTPUT_DIR: /tmp
    PRETRAINED_MODEL: /tmp/model_save
    TRAIN_SCRIPT: /workspace/pytorch-bert-large-training/models/language_modeling/pytorch/bert_large/training/run_pretrain_mlperf.py
    EVAL_SCRIPT: /workspace/pytorch-bert-large-training/quickstart/transformers/examples/legacy/question-answering/run_squad.py
    DATASET_DIR: /pytorch/tfrecord_dir/bert_results4_dataset/
  volumes:
    - src: /pytorch/enwiki-20200101/bert_config.json
      dst: /pytorch/enwiki-20200101/bert_config.json
    - src: /tmp/model_save
      dst: /tmp/model_save
    - src: /pytorch/tfrecord_dir/bert_results4_dataset/
      dst: /pytorch/tfrecord_dir/bert_results4_dataset/
bf32-training-phase1:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-training
  cmd: quickstart/run_bert_pretrain_phase1.sh bf32
  shm-size: 8G
  env:
    PRECISION: bf32
    BERT_MODEL_CONFIG: /pytorch/enwiki-20200101/bert_config.json
    OUTPUT_DIR: /tmp
    TRAIN_SCRIPT: /workspace/pytorch-bert-large-training/models/language_modeling/pytorch/bert_large/training/run_pretrain_mlperf.py
    EVAL_SCRIPT: /workspace/pytorch-bert-large-training/quickstart/transformers/examples/legacy/question-answering/run_squad.py
    DATASET_DIR: /pytorch/tfrecord_dir/bert_results4_dataset/
  volumes:
    - src: /pytorch/enwiki-20200101/bert_config.json
      dst: /pytorch/enwiki-20200101/bert_config.json
    - src: /tmp
      dst: /tmp
    - src: /pytorch/tfrecord_dir/bert_results4_dataset/
      dst: /pytorch/tfrecord_dir/bert_results4_dataset/
bf32-training-phase2:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-training
  cmd: quickstart/run_bert_pretrain_phase2.sh bf32
  shm-size: 8G
  env:
    PRECISION: bf32
    BERT_MODEL_CONFIG: /pytorch/enwiki-20200101/bert_config.json
    OUTPUT_DIR: /tmp
    PRETRAINED_MODEL: /tmp/model_save
    TRAIN_SCRIPT: /workspace/pytorch-bert-large-training/models/language_modeling/pytorch/bert_large/training/run_pretrain_mlperf.py
    EVAL_SCRIPT: /workspace/pytorch-bert-large-training/quickstart/transformers/examples/legacy/question-answering/run_squad.py
    DATASET_DIR: /pytorch/tfrecord_dir/bert_results4_dataset/
  volumes:
    - src: /pytorch/enwiki-20200101/bert_config.json
      dst: /pytorch/enwiki-20200101/bert_config.json
    - src: /tmp/model_save
      dst: /tmp/model_save
    - src: /pytorch/tfrecord_dir/bert_results4_dataset/
      dst: /pytorch/tfrecord_dir/bert_results4_dataset/
avx-fp32-training-phase1:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-training
  cmd: quickstart/run_bert_pretrain_phase1.sh avx-fp32
  shm-size: 8G
  env:
    PRECISION: avx-fp32
    BERT_MODEL_CONFIG: /pytorch/enwiki-20200101/bert_config.json
    OUTPUT_DIR: /tmp
    TRAIN_SCRIPT: /workspace/pytorch-bert-large-training/models/language_modeling/pytorch/bert_large/training/run_pretrain_mlperf.py
    EVAL_SCRIPT: /workspace/pytorch-bert-large-training/quickstart/transformers/examples/legacy/question-answering/run_squad.py
    DATASET_DIR: /pytorch/tfrecord_dir/bert_results4_dataset/
  volumes:
    - src: /pytorch/enwiki-20200101/bert_config.json
      dst: /pytorch/enwiki-20200101/bert_config.json
    - src: /tmp
      dst: /tmp
    - src: /pytorch/tfrecord_dir/bert_results4_dataset/
      dst: /pytorch/tfrecord_dir/bert_results4_dataset/
avx-fp32-training-phase2:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${BASE_IMAGE_NAME:-ubuntu}-${BASE_IMAGE_TAG:-22.04}-language-modeling-bert-large-training
  cmd: quickstart/run_bert_pretrain_phase2.sh avx-fp32
  shm-size: 8G
  env:
    PRECISION: avx-fp32
    BERT_MODEL_CONFIG: /pytorch/enwiki-20200101/bert_config.json
    OUTPUT_DIR: /tmp
    PRETRAINED_MODEL: /tmp/model_save
    TRAIN_SCRIPT: /workspace/pytorch-bert-large-training/models/language_modeling/pytorch/bert_large/training/run_pretrain_mlperf.py
    EVAL_SCRIPT: /workspace/pytorch-bert-large-training/quickstart/transformers/examples/legacy/question-answering/run_squad.py
    DATASET_DIR: /pytorch/tfrecord_dir/bert_results4_dataset/
  volumes:
    - src: /pytorch/enwiki-20200101/bert_config.json
      dst: /pytorch/enwiki-20200101/bert_config.json
    - src: /tmp/model_save
      dst: /tmp/model_save
    - src: /pytorch/tfrecord_dir/bert_results4_dataset/
      dst: /pytorch/tfrecord_dir/bert_results4_dataset/
