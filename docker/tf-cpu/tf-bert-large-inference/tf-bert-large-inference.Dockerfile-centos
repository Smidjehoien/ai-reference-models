# Copyright (c) 2020-2021 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

ARG TF_BASE_IMAGE=intel/intel-optimized-tensorflow-avx512

ARG TF_BASE_TAG=latest

FROM ${TF_BASE_IMAGE}:${TF_BASE_TAG}

WORKDIR /workspace/tf-bert-large-inference

RUN yum update -y && \
    yum install -y numactl unzip wget ca-certificates && \
    yum clean all

RUN mkdir -p /workspace/tcmalloc && \
    cd /workspace/tcmalloc && \
    wget https://github.com/gperftools/gperftools/releases/download/gperftools-2.7.90/gperftools-2.7.90.tar.gz && \
    tar -xzf gperftools-2.7.90.tar.gz && \
    rm gperftools-2.7.90.tar.gz && \
    cd gperftools-2.7.90 && \
    ./configure --prefix=/workspace/tcmalloc/install && \
    make && \
    make install && \
    cd /workspace && \
    rm -rf gperftools-2.7.90

ENV LD_LIBRARY_PATH="/workspace/tcmalloc/install/lib/":${LD_LIBRARY_PATH}
ENV LD_PRELOAD=/workspace/tcmalloc/install/lib/libtcmalloc.so.4

COPY benchmarks benchmarks
COPY models/common models/common
COPY models/language_modeling/tensorflow/bert_large/inference models/language_modeling/tensorflow/bert_large/inference
COPY quickstart/common quickstart/common 
COPY quickstart/language_modeling/tensorflow/bert_large/inference/cpu/inference_realtime_multi_instance.sh quickstart/inference_realtime.sh
COPY quickstart/language_modeling/tensorflow/bert_large/inference/cpu/inference_realtime_weight_sharing.sh quickstart/inference_realtime_weight_sharing.sh
COPY quickstart/language_modeling/tensorflow/bert_large/inference/cpu/inference_throughput_multi_instance.sh quickstart/inference_throughput.sh
COPY quickstart/language_modeling/tensorflow/bert_large/inference/cpu/accuracy.sh quickstart/accuracy.sh

RUN python -m pip install pillow==10.2.0 \
        cryptography==42.0.4 \
        setuptools==65.5.1 \
        werkzeug==3.0.1
        
COPY LICENSE license/LICENSE
COPY third_party license/third_party

RUN mkdir -p /workspace/tf-bert-large-inference/pretrained_model && \ 
    wget -qO /workspace/tf-bert-large-inference/pretrained_model/bert_large_fp32_pretrained_model.pb https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/fp32_bert_squad.pb && \
    wget -qO /workspace/tf-bert-large-inference/pretrained_model/bert_large_bfloat16_pretrained_model.pb https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/optimized_bf16_bert.pb && \
    wget -qO /workspace/tf-bert-large-inference/pretrained_model/bert_large_int8_pretrained_model.pb https://storage.googleapis.com/intel-optimized-tensorflow/models/2_10_0/per_channel_opt_int8_bf16_bert.pb && \
    wget -qO /workspace/tf-bert-large-inference/bert_large_checkpoints.zip https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_8/bert_large_checkpoints.zip && \
    unzip /workspace/tf-bert-large-inference/bert_large_checkpoints.zip -d /workspace/tf-bert-large-inference/pretrained_model/ && \
    rm /workspace/tf-bert-large-inference/bert_large_checkpoints.zip
