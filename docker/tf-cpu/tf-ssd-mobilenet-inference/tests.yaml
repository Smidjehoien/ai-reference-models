SSD-MobileNet Accuracy int8:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-object-detection-tf-ssd-mobilenet-inference
  cmd: /bin/bash quickstart/accuracy.sh --num-intra-threads 56  --num-inter-threads 1
  env:
    DATASET_DIR: /tf_dataset/dataset/SSDMobilenet
    OUTPUT_DIR: /output/tf-ssd-mobilenet-inference/accuracy_int8
    PRECISION: int8
    BATCH_SIZE: '1'
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/SSDMobilenet
    dst: /tf_dataset/dataset/SSDMobilenet
  - src: $PWD/output/tf-ssd-mobilenet-inference/accuracy_int8
    dst: /output/tf-ssd-mobilenet-inference/accuracy_int8
SSD-MobileNet Accuracy fp32:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-object-detection-tf-ssd-mobilenet-inference
  cmd: /bin/bash quickstart/accuracy.sh --num-intra-threads 56  --num-inter-threads 1
  env:
    DATASET_DIR: /tf_dataset/dataset/SSDMobilenet
    OUTPUT_DIR: /output/tf-ssd-mobilenet-inference/accuracy_fp32
    PRECISION: fp32
    BATCH_SIZE: '1'
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/SSDMobilenet
    dst: /tf_dataset/dataset/SSDMobilenet
  - src: $PWD/output/tf-ssd-mobilenet-inference/accuracy_fp32
    dst: /output/tf-ssd-mobilenet-inference/accuracy_fp32
SSD-MobileNet Accuracy bf16:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-object-detection-tf-ssd-mobilenet-inference
  cmd: /bin/bash quickstart/accuracy.sh --num-intra-threads 56  --num-inter-threads 1
  env:
    DATASET_DIR: /tf_dataset/dataset/SSDMobilenet
    OUTPUT_DIR: /output/tf-ssd-mobilenet-inference/accuracy_bf16
    PRECISION: bfloat16
    TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_ALLOWLIST_ADD: BiasAdd,Relu6,Mul,AddV2
    TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_INFERLIST_REMOVE: BiasAdd,AddV2,Mul
    TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_CLEARLIST_REMOVE: Relu6
    BATCH_SIZE: '1'
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: /tf_dataset/dataset/SSDMobilenet
    dst: /tf_dataset/dataset/SSDMobilenet
  - src: $PWD/output/tf-ssd-mobilenet-inference/accuracy_bf16
    dst: /output/tf-ssd-mobilenet-inference/accuracy_bf16
SSD-MobileNet Realtime Inference fp32:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-object-detection-tf-ssd-mobilenet-inference
  cmd: /bin/bash quickstart/inference_realtime_multi_instance.sh --benchmark-only --num-intra-threads 4 --num-inter-threads 1 --verbose
  env:
    DATASET_DIR: ""
    OUTPUT_DIR: /output/tf-ssd-mobilenet-inference/realtime_fp32
    BATCH_SIZE: '1'
    PRECISION: fp32
    OMP_NUM_THREADS: '4'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: $PWD/output/tf-ssd-mobilenet-inference/realtime_fp32
    dst: /output/tf-ssd-mobilenet-inference/realtime_fp32
SSD-MobileNet Realtime Inference bf16:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-object-detection-tf-ssd-mobilenet-inference
  cmd: /bin/bash quickstart/inference_realtime_multi_instance.sh --benchmark-only --num-intra-threads 4 --num-inter-threads 1 --verbose
  env:
    DATASET_DIR: ""
    OUTPUT_DIR: /output/tf-ssd-mobilenet-inference/realtime_bf16
    BATCH_SIZE: '1'
    PRECISION: bfloat16
    OMP_NUM_THREADS: '4'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
    TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_ALLOWLIST_ADD: BiasAdd,Relu6,Mul,AddV2
    TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_INFERLIST_REMOVE: BiasAdd,AddV2,Mul
    TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_CLEARLIST_REMOVE: Relu6
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: $PWD/output/tf-ssd-mobilenet-inference/realtime_bf16
    dst: /output/tf-ssd-mobilenet-inference/realtime_bf16
SSD-MobileNet Realtime Inference int8:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-object-detection-tf-ssd-mobilenet-inference
  cmd: /bin/bash quickstart/inference_realtime_multi_instance.sh --benchmark-only --num-intra-threads 4 --num-inter-threads 1 --verbose
  env:
    DATASET_DIR: ""
    OUTPUT_DIR: /output/tf-ssd-mobilenet-inference/realtime_int8
    BATCH_SIZE: '1'
    PRECISION: int8
    OMP_NUM_THREADS: '4'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: $PWD/output/tf-ssd-mobilenet-inference/realtime_int8
    dst: /output/tf-ssd-mobilenet-inference/realtime_int8
SSD-MobileNet Throughput fp32:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-object-detection-tf-ssd-mobilenet-inference
  cmd: /bin/bash quickstart/inference_throughput_multi_instance.sh --benchmark-only --num-intra-threads 64 --num-inter-threads 1 --verbose
  env:
    DATASET_DIR: ""
    OUTPUT_DIR: /output/tf-ssd-mobilenet-inference/throughput_fp32
    BATCH_SIZE: '448'
    PRECISION: fp32
    OMP_NUM_THREADS: '64'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: $PWD/output/tf-ssd-mobilenet-inference/throughput_fp32
    dst: /output/tf-ssd-mobilenet-inference/throughput_fp32
SSD-MobileNet Throughput bf16:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-object-detection-tf-ssd-mobilenet-inference
  cmd: /bin/bash quickstart/inference_throughput_multi_instance.sh --benchmark-only --num-intra-threads 64 --num-inter-threads 1 --verbose
  env:
    DATASET_DIR: ""
    OUTPUT_DIR: /output/tf-ssd-mobilenet-inference/throughput_bf16
    BATCH_SIZE: '448'
    PRECISION: bfloat16
    OMP_NUM_THREADS: '64'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
    TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_ALLOWLIST_ADD: BiasAdd,Relu6,Mul,AddV2
    TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_INFERLIST_REMOVE: BiasAdd,AddV2,Mul
    TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_CLEARLIST_REMOVE: Relu6
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: $PWD/output/tf-ssd-mobilenet-inference/throughput_bf16
    dst: /output/tf-ssd-mobilenet-inference/throughput_bf16
SSD-MobileNet Throughput int8:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-${OS:-ubuntu}-object-detection-tf-ssd-mobilenet-inference
  cmd: /bin/bash quickstart/inference_throughput_multi_instance.sh --benchmark-only --num-intra-threads 64 --num-inter-threads 1 --verbose
  env:
    DATASET_DIR: ""
    OUTPUT_DIR: /output/tf-ssd-mobilenet-inference/throughput_int8
    BATCH_SIZE: '448'
    PRECISION: int8
    OMP_NUM_THREADS: '64'
    KMP_AFFINITY: granularity=fine,verbose,compact,1,0
  shm_size: 8G
  privileged: true
  init: true
  volumes:
  - src: $PWD/output/tf-ssd-mobilenet-inference/throughput_int8
    dst: /output/tf-ssd-mobilenet-inference/throughput_int8
