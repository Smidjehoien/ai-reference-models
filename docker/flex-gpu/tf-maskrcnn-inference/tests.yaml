170-online-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-image-segmentation-tf-flex-gpu-maskrcnn-inference
  cmd: quickstart/inference.sh
  ipc: host
  device: /dev/dri
  env:
    BATCH_SIZE: '1'
    PRECISION: fp16
    GPU_TYPE: flex_170
    DATASET_DIR: /dataset/maskrcnn/coco
    OUTPUT_DIR: /tmp
  volumes:
    - src: /dataset/maskrcnn/coco
      dst: /dataset/maskrcnn/coco
170-batch-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-image-segmentation-tf-flex-gpu-maskrcnn-inference
  cmd: quickstart/inference.sh
  ipc: host
  device: /dev/dri
  env:
    BATCH_SIZE: '16'
    PRECISION: fp16
    GPU_TYPE: flex_170
    DATASET_DIR: /dataset/maskrcnn/coco
    OUTPUT_DIR: /tmp
  volumes:
    - src: /dataset/maskrcnn/coco
      dst: /dataset/maskrcnn/coco
140-online-inference:
  img: ${REGISTRY}/aiops/mlops-ci:b-${GITHUB_RUN_NUMBER:-0}-image-segmentation-tf-flex-gpu-maskrcnn-inference
  cmd: quickstart/inference.sh
  ipc: host
  device: /dev/dri
  env:
    BATCH_SIZE: '1'
    PRECISION: fp16
    GPU_TYPE: flex_140
    DATASET_DIR: /dataset/maskrcnn/coco
    OUTPUT_DIR: /tmp
  volumes:
    - src: /dataset/maskrcnn/coco
      dst: /dataset/maskrcnn/coco
